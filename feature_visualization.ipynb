{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the different Explanations\n",
    "\n",
    "- for each feature find images where three annotators agreed on the feature and the feature location\n",
    "- extract images of the tissue and compare to find similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mittmann/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "import omegaconf\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "\n",
    "from src.gleason_data import GleasonX\n",
    "from src.augmentations import normalize_only_transform\n",
    "from src.lightning_modul import LitSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"/hdd_seahawk/00_data\") / \"04 GleasonXAI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating masks on the fly. Very slow!\n"
     ]
    }
   ],
   "source": [
    "base_path = Path(os.environ['DATASET_LOCATION']) / \"GleasonXAI\"\n",
    "label_level = 1\n",
    "data = GleasonX(path=base_path, split='all', scaling=\"MicronsCalibrated\", transforms=None,\n",
    "                                 label_level=label_level, create_seg_masks=True, explanation_file=\"final_filtered_explanations_df.csv\", data_split=[0.7, 0.15, 0.15], tissue_mask_kwargs={\"open\": False, \"close\": False, \"flood\": False})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all three rater agreeing areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CREATE FOLDERS --\n",
    "tissue_image_path = out_path / \"tissue_extraction\"\n",
    "tissue_image_path.mkdir(exist_ok=True)\n",
    "\n",
    "for name in data.classes_named:\n",
    "    class_dir = tissue_image_path / f\"{name}\"\n",
    "    class_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {v: k for k, v in data.classes_number_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1015 [00:04<23:35,  1.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m img, mask, background_mask \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(i, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m np_masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mint8(np\u001b[38;5;241m.\u001b[39marray(mask))\n\u001b[0;32m----> 5\u001b[0m agreement_count \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_number, class_agreement_array \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agreement_count):\n\u001b[1;32m      8\u001b[0m     three_agreement_mask \u001b[38;5;241m=\u001b[39m (class_agreement_array \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m)\u001b[38;5;66;03m#.any(axis=0)\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/numpy/lib/shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m buff[ind0] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[0;32m--> 402\u001b[0m     buff[ind] \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     buff \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39m__array_wrap__(buff)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    img, mask, background_mask = data.get(i, False)\n",
    "\n",
    "    np_masks = np.int8(np.array(mask))\n",
    "    agreement_count = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=np_masks)\n",
    "\n",
    "    for class_number, class_agreement_array in enumerate(agreement_count):\n",
    "        three_agreement_mask = (class_agreement_array == 3)#.any(axis=0)\n",
    "        if not np.any(three_agreement_mask):\n",
    "            continue\n",
    "        else:\n",
    "            area_of_interest = np.where(three_agreement_mask[..., None], img, 0)  # Keep pixels where three agree, set others to black\n",
    "            output_image = Image.fromarray(area_of_interest)\n",
    "            output_image.save(tissue_image_path / name_dict[class_number] / f\"img_{i}.png\", dpi=(1000, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect confident areas for each feature of GleasonXAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [Path(f\"GleasonFinal2/label_level1/SoftDiceBalanced-{i}/version_0/checkpoints/best_model.ckpt\") for i in [1, 2, 3]]\n",
    "\n",
    "model_config = base_path / \"GleasonFinal2\"/\"label_level1\"/\"SoftDiceBalanced-1\"/\"version_0\"/\"logs\"/\"config.yaml\"\n",
    "config = omegaconf.OmegaConf.load(model_config)\n",
    "\n",
    "preds_paths = []\n",
    "for path in model_paths:\n",
    "    assert (base_path/path).exists(), f\"Could not find {str(base_path/path)}\"\n",
    "\n",
    "TRANSFORM = normalize_only_transform\n",
    "tissue_mask_kwargs =  {\"open\": False, \"close\": False, \"flood\": False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "net = hydra.utils.instantiate(config.model, classes=data.num_classes)\n",
    "SLIDING_WINDOW_INFERER = SlidingWindowInferer(roi_size=(512, 512), sw_batch_size=1, overlap=0.5, mode=\"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mittmann/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/utils.py:55: The loaded checkpoint was produced with Lightning v2.2.0.post0, which is newer than your current Lightning version: v2.1.3\n",
      "/home/mittmann/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "model1 = LitSegmenter.load_from_checkpoint(str(base_path / model_paths[0]), map_location=device)\n",
    "model2 = LitSegmenter.load_from_checkpoint(str(base_path / model_paths[1]), map_location=device)\n",
    "model3 = LitSegmenter.load_from_checkpoint(str(base_path / model_paths[2]), map_location=device)\n",
    "\n",
    "models = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output(model, img, device=\"cpu\", label_remapping=None, inferer=SLIDING_WINDOW_INFERER, transform=TRANSFORM):\n",
    "    model.eval()\n",
    "\n",
    "    img = TRANSFORM(image=img)['image']\n",
    "    \n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = tt.functional.to_tensor(img)\n",
    "    \n",
    "\n",
    "    if len(img.size()) == 3:\n",
    "        no_batch_input = True\n",
    "        img = img.unsqueeze(0)\n",
    "    else:\n",
    "        no_batch_input = False\n",
    "\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if inferer is not None:\n",
    "            out = inferer(img, model)\n",
    "        else:\n",
    "            out = model(img)\n",
    "\n",
    "    # Move back and strip batch_dim\n",
    "    out = out.cpu()\n",
    "\n",
    "    if label_remapping is not None:\n",
    "        out = label_remapping(out)\n",
    "\n",
    "    if no_batch_input:\n",
    "        out = out[0, ...]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_output(example_img):\n",
    "    preds_ensemble = generate_model_output(models[0], example_img, device, None, SLIDING_WINDOW_INFERER, TRANSFORM)\n",
    "\n",
    "    for model in models[1:]:\n",
    "        out = generate_model_output(model, example_img, device, None, SLIDING_WINDOW_INFERER, TRANSFORM)\n",
    "        preds_ensemble += out\n",
    "\n",
    "    preds_ensemble = torch.nn.functional.softmax(preds_ensemble, dim=0)\n",
    "    return np.array(preds_ensemble.argmax(dim=0)).astype(np.uint8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tissue_path = out_path / \"predicted_tissue\"\n",
    "predicted_tissue_path.mkdir(exist_ok=True)\n",
    "\n",
    "for name in data.classes_named:\n",
    "    class_dir = predicted_tissue_path / f\"{name}\"\n",
    "    class_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1015 [00:29<2:03:04,  7.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data))):\n\u001b[1;32m      2\u001b[0m     img, _, background_mask \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(i, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     np_seg \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ensemble_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     mask_np_seg \u001b[38;5;241m=\u001b[39m np_seg \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m     mask_np_seg[background_mask\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mgenerate_ensemble_output\u001b[0;34m(example_img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_ensemble_output\u001b[39m(example_img):\n\u001b[0;32m----> 2\u001b[0m     preds_ensemble \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_model_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSLIDING_WINDOW_INFERER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRANSFORM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m      5\u001b[0m         out \u001b[38;5;241m=\u001b[39m generate_model_output(model, example_img, device, \u001b[38;5;28;01mNone\u001b[39;00m, SLIDING_WINDOW_INFERER, TRANSFORM)\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mgenerate_model_output\u001b[0;34m(model, img, device, label_remapping, inferer, transform)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inferer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43minferer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(img)\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/monai/inferers/inferer.py:515\u001b[0m, in \u001b[0;36mSlidingWindowInferer.__call__\u001b[0;34m(self, inputs, network, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_thresh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_thresh:\n\u001b[1;32m    513\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# stitch in cpu memory if image is too large\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msliding_window_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msw_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msw_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_weight_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_coord\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/monai/inferers/utils.py:229\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, process_fn, buffer_steps, buffer_dim, with_coord, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     seg_prob_out \u001b[38;5;241m=\u001b[39m predictor(win_data, unravel_slice, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# batched patch\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     seg_prob_out \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwin_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# batched patch\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# convert seg_prob_out to tuple seg_tuple, this does not allocate new memory.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m dict_keys, seg_tuple \u001b[38;5;241m=\u001b[39m _flatten_struct(seg_prob_out)\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/03 Code/04 Gleason/revision/GleasonXAI-revisison/src/lightning_modul.py:351\u001b[0m, in \u001b[0;36mLitSegmenter.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 351\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/segmentation_models_pytorch/base/model.py:29\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m---> 29\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m     32\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/segmentation_models_pytorch/encoders/efficientnet.py:73\u001b[0m, in \u001b[0;36mEfficientNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m             drop_connect \u001b[38;5;241m=\u001b[39m drop_connect_rate \u001b[38;5;241m*\u001b[39m block_number \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocks)\n\u001b[1;32m     72\u001b[0m             block_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 73\u001b[0m             x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_connect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/efficientnet_pytorch/model.py:107\u001b[0m, in \u001b[0;36mMBConvBlock.forward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    105\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_conv(inputs)\n\u001b[1;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn0(x)\n\u001b[0;32m--> 107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_swish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depthwise_conv(x)\n\u001b[1;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn1(x)\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/efficientnet_pytorch/utils.py:80\u001b[0m, in \u001b[0;36mMemoryEfficientSwish.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSwishImplementation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/efficientnet_pytorch/utils.py:67\u001b[0m, in \u001b[0;36mSwishImplementation.forward\u001b[0;34m(ctx, i)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, i):\n\u001b[0;32m---> 67\u001b[0m     result \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(i)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    img, _, background_mask = data.get(i, False)\n",
    "\n",
    "    np_seg = generate_ensemble_output(img)\n",
    "    mask_np_seg = np_seg + 1\n",
    "    mask_np_seg[background_mask.astype(bool)] = 0\n",
    "\n",
    "\n",
    "    for class_number in range(1, data.num_classes):\n",
    "        class_mask = (mask_np_seg == class_number + 1)\n",
    "        class_with_bg_mask = (np_seg == class_number) \n",
    "        if not np.any(class_mask): # check if class is in foreground\n",
    "            continue               # if not, continue, else extract all area (even in background)\n",
    "        area_of_interest = np.where(class_with_bg_mask[..., None], img, 0)\n",
    "        output_image = Image.fromarray(area_of_interest)\n",
    "        output_image.save(predicted_tissue_path / name_dict[class_number] / f\"img_{i}.png\", dpi=(1000, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect overlap in annotations of classes which are often confused by the GleasonXAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant combinations: \n",
    "annotated and predicted (see Fig. 6)\n",
    "- compressed glands [2] and individual glands [1]\n",
    "- glomeruloid [5] and cribriform [4]\n",
    "- single cells [7] and cords [8]\n",
    "- comedonecrosis [9] and cribriform [4]\n",
    "\n",
    "NEW: annotated and predicted\n",
    "- compressed glands 2 and individual glands 1\n",
    "- glomeruloid 5 and poorly formed glands 3\n",
    "- single cells 7 and cords 8\n",
    "- comedonecrosis 9 and cribriform 4\n",
    "- comedonecrosis 9 and group of tumor cells 6\n",
    "- poorly formed 3 and cribriform 4\n",
    "- poorly formed 3 and cords 8\n",
    "- poorly formed 3 and individual glands 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CREATE FOLDERS --\n",
    "# 2 1:\n",
    "compressed_individual = out_path / \"compressed_individual\"\n",
    "compressed_individual.mkdir(exist_ok=True)\n",
    "\n",
    "# 5 4:\n",
    "glomeruloid_cribriform = out_path / \"glomeruloid_cribriform\"\n",
    "glomeruloid_cribriform.mkdir(exist_ok=True)\n",
    "\n",
    "# 7 8:\n",
    "single_cords = out_path / \"single_cords\"\n",
    "single_cords.mkdir(exist_ok=True)\n",
    "\n",
    "# 9 4:\n",
    "comedo_cribri_path = out_path / \"comedonecrosis_cribriform\"\n",
    "comedo_cribri_path.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of images with overlap / confusion between the annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set confused features (names) and path into which the images of the overlapped labels are safed into\n",
    "annotate_val = 3\n",
    "predict_val = 1\n",
    "image_path = comedo_cribri_path\n",
    "\n",
    "#reset counters\n",
    "predict_counter = 0\n",
    "annotate_counter = 0\n",
    "both_counter = 0\n",
    "overlap_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [01:20<00:00, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poorly formed and fused glands: 696\n",
      "variable sized well-formed individual and discrete glands: 547\n",
      "Overlap: 307\n",
      "Both: 368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    img, mask, _ = data.get(i, False)\n",
    "\n",
    "    np_masks = np.array(mask)\n",
    "    # agreement_count = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=np_masks)\n",
    "    confusion_mask = (np_masks == predict_val).any(axis=0) & (np_masks == annotate_val).any(axis=0)\n",
    "\n",
    "    if np.any(np_masks == predict_val):\n",
    "        predict_counter += 1\n",
    "    \n",
    "    if np.any(np_masks == annotate_val):\n",
    "        annotate_counter += 1\n",
    "    \n",
    "    if np.any(np_masks == predict_val) and np.any(np_masks == annotate_val):\n",
    "        both_counter += 1\n",
    "\n",
    "    if not np.any(confusion_mask):\n",
    "        continue\n",
    "\n",
    "    overlap_counter += 1\n",
    "    area_of_interest = np.where(confusion_mask[..., None], img, 0)  # Keep pixels where three agree, set others to black\n",
    "    # output_image = Image.fromarray(area_of_interest)\n",
    "    # output_image.save(image_path / f\"img_{i}.png\")\n",
    "\n",
    "print(f\"{name_dict[annotate_val]}:\", annotate_counter)\n",
    "print(f\"{name_dict[predict_val]}:\", predict_counter)\n",
    "print(\"Overlap:\", overlap_counter)\n",
    "print(\"Both:\", both_counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "compressed or angular discrete glands: 260  \n",
    "variable sized well-formed individual and discrete glands: 547  \n",
    "Overlap: 240  \n",
    "Both: 256  \n",
    "--> 260 images with Compressed glands  \n",
    "--> in 240 (92, 31%) of those images there is overlap of the classes according to the annotators   \n",
    "    (i.e. at least one pixel where one annotator said cribriform and a different one compressed glands)  \n",
    "\n",
    "---\n",
    "\n",
    "Glomeruloid glands: 55\n",
    "poorly formed and fused glands: 696\n",
    "Overlap: 44\n",
    "Both: 52\n",
    "--> 80%\n",
    "\n",
    "---\n",
    "\n",
    "Glomeruloid glands: 55  \n",
    "Cribriform glands: 388  \n",
    "Overlap: 26  \n",
    "Both: 34  \n",
    "--> 47,27%  \n",
    "\n",
    "---\n",
    "\n",
    "single cells: 104  \n",
    "cords: 214  \n",
    "Overlap: 60  \n",
    "Both: 74  \n",
    "--> 57,69%  \n",
    "\n",
    "---\n",
    "\n",
    "presence of comedonecrosis: 82  \n",
    "Cribriform glands: 388  \n",
    "Overlap: 21  \n",
    "Both: 29  \n",
    "--> 35,37%  \n",
    "\n",
    "---\n",
    "\n",
    "presence of comedonecrosis: 82  \n",
    "solid groups of tumor cells: 228  \n",
    "Overlap: 63  \n",
    "Both: 66  \n",
    "\n",
    "---\n",
    "\n",
    "poorly formed and fused glands: 696  \n",
    "Cribriform glands: 388  \n",
    "Overlap: 314  \n",
    "Both: 339  \n",
    "\n",
    "---\n",
    "\n",
    "poorly formed and fused glands: 696  \n",
    "cords: 214  \n",
    "Overlap: 126  \n",
    "Both: 139  \n",
    "\n",
    "---\n",
    "\n",
    "poorly formed and fused glands: 696  \n",
    "variable sized well-formed individual and discrete glands: 547  \n",
    "Overlap: 307  \n",
    "Both: 368  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of pixels with overlap / confusion in the annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "annotated and predicted\n",
    "- compressed glands 2 and individual glands 1\n",
    "- glomeruloid 5 and cribriform 4\n",
    "- single cells 7 and cords 8\n",
    "- comedonecrosis 9 and cribriform 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [01:20<00:00, 12.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poorly formed and fused glands: 154054357\n",
      "Benign: 469102418\n",
      "Overlap: 55994491\n",
      "\n",
      "0.3634722969893023 of poorly formed and fused glands overlapped by Benign\n",
      "0.11936517240463254 of Benign overlapped by poorly formed and fused glands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_counter = 0\n",
    "annotate_counter = 0\n",
    "overlap_counter = 0\n",
    "\n",
    "annotate_val = 3\n",
    "predict_val = 0\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    img, mask, _ = data.get(i, False)\n",
    "\n",
    "    np_masks = np.array(mask)\n",
    "    # agreement_count = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=np_masks)\n",
    "    confusion_mask = (np_masks == predict_val).any(axis=0) & (np_masks == annotate_val).any(axis=0)\n",
    "\n",
    "    if np.any(np_masks == predict_val):\n",
    "        predict_counter += np.sum((np_masks == predict_val).any(axis=0))\n",
    "    \n",
    "    if np.any(np_masks == annotate_val):\n",
    "        annotate_counter += np.sum((np_masks == annotate_val).any(axis=0))\n",
    "\n",
    "    if not np.any(confusion_mask):\n",
    "        continue\n",
    "\n",
    "    overlap_counter += np.sum(confusion_mask)\n",
    "\n",
    "print(f\"{name_dict[annotate_val]}:\", annotate_counter)\n",
    "print(f\"{name_dict[predict_val]}:\", predict_counter)\n",
    "print(\"Overlap:\", overlap_counter)\n",
    "print(\"\")\n",
    "print(f\"{overlap_counter / annotate_counter} of {name_dict[annotate_val]} overlapped by {name_dict[predict_val]}\")\n",
    "print(f\"{overlap_counter / predict_counter} of {name_dict[predict_val]} overlapped by {name_dict[annotate_val]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76.447% of compressed or angular discrete glands overlapped by variable sized well-formed individual and discrete glands  \n",
    "12.808% of variable sized well-formed individual discrete glands are overlapped by compressed or angular discrete glands  \n",
    "\n",
    "64.265% of Glomeruloid glands overlapped by poorly formed and fused glands  \n",
    "04.240% of poorly formed and fused glands overlapped by Glomeruloid glands  \n",
    "\n",
    "35.134% of Glomeruloid glands overlapped by Cribriform glands  \n",
    "03.996% of Cribriform glands overlapped by Glomeruloid glands  \n",
    "\n",
    "38.919% of single cells overlapped by cords  \n",
    "09.898% of cords overlapped by single cells  \n",
    "\n",
    "09.487% of presence of comedonecrosis overlapped by Cribriform glands  \n",
    "02.421% of Cribriform glands overlapped by presence of comedonecrosis  \n",
    "\n",
    "55.327% of presence of comedonecrosis overlapped by solid groups of tumor cells  \n",
    "28.968% of solid groups of tumor cells overlapped by presence of comedonecrosis  \n",
    "\n",
    "\n",
    "55.327% of presence of comedonecrosis overlapped by solid groups of tumor cells  \n",
    "28.968% of solid groups of tumor cells overlapped by presence of comedonecrosis  \n",
    "\n",
    "03.329% of Cribriform glands overlapped by solid groups of tumor cells  \n",
    "06.831% of solid groups of tumor cells overlapped by Cribriform glands  \n",
    "\n",
    "32.743% of poorly formed and fused glands overlapped by Cribriform glands  \n",
    "56.456% of Cribriform glands overlapped by poorly formed and fused glands  \n",
    "\n",
    "05.662 of poorly formed and fused glands overlapped by cords  \n",
    "16.894 of cords overlapped by poorly formed and fused glands  \n",
    "\n",
    "08.687% of poorly formed and fused glands overlapped by variable sized well-formed individual and discrete glands  \n",
    "14.691% of variable sized well-formed individual and discrete glands overlapped by poorly formed and fused glands  \n",
    "\n",
    "\n",
    "--> 76.447% of the pixels labeled with compressed glands were also labeled with individual glands by at least one annotator  \n",
    "--> 64.265% of the pixels labeled with Glomeruloid glands were also labeled with poorly formed and fused glands by at least one annotator  \n",
    "--> 35.134% of the pixels labeled with Glomeruloid glands  were also labeled with Cribriform glands by at least one annotator  \n",
    "--> 38.919% of the pixels labeled with single cells  were also labeled with cords by at least one annotator  \n",
    "--> 55.327% of the pixels labeled with presence of comedonecrosis were also labeled with solid groups of tumor cells by at least one annotator  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> open question: why is comedonecrosis predicted as cribriform?\n",
    "\n",
    "comedonecrosis to groups of tumor cells is clear (high overlap like the others)\n",
    "\n",
    "comedo necrosis occurs in solid or cribriform glands.\n",
    "--> most likely the comedonecrosis is within a label of cribriform and was therefore overwritten\n",
    "--> the removal of background might remove the comedonecrosis  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Comedonecrosis to background: How much is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 0:\n",
    "comedo_background_path = out_path / \"comedonecrosis_background\"\n",
    "comedo_background_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [01:18<00:00, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presence of comedonecrosis: 22802625\n",
      "Benign: 0\n",
      "Overlap: 1747380\n",
      "\n",
      "0.07663065107635635 of presence of comedonecrosis overlapped by background\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_counter = 0\n",
    "annotate_counter = 0\n",
    "overlap_counter = 0\n",
    "\n",
    "annotate_val = 9\n",
    "predict_val = 0\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    img, mask, bg_mask = data.get(i, False)\n",
    "\n",
    "    np_masks = np.array(mask)\n",
    "    # agreement_count = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=np_masks)\n",
    "    confusion_mask = (np_masks == annotate_val).any(axis=0) & bg_mask\n",
    "    \n",
    "    if np.any(np_masks == annotate_val):\n",
    "        annotate_counter += np.sum((np_masks == annotate_val).any(axis=0))\n",
    "\n",
    "    if not np.any(confusion_mask):\n",
    "        continue\n",
    "\n",
    "    overlap_counter += np.sum(confusion_mask)\n",
    "\n",
    "print(f\"{name_dict[annotate_val]}:\", annotate_counter)\n",
    "print(f\"{name_dict[predict_val]}:\", predict_counter)\n",
    "print(\"Overlap:\", overlap_counter)\n",
    "print(\"\")\n",
    "print(f\"{overlap_counter / annotate_counter} of {name_dict[annotate_val]} overlapped by background\")\n",
    "# print(f\"{overlap_counter / predict_counter} of {name_dict[predict_val]} overlapped by {name_dict[annotate_val]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GP 5 prediction in GP 5 annotated test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create test set\n",
    "- for every image in test set: check for three rater annotation of any of the GP 5 classes\n",
    "- if GP5 class: check if GP5 predicted\n",
    "- check overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating masks on the fly. Very slow!\n"
     ]
    }
   ],
   "source": [
    "test_data = GleasonX(path=base_path, split='test', scaling=\"MicronsCalibrated\", transforms=None,\n",
    "                                 label_level=label_level, create_seg_masks=True, explanation_file=\"final_filtered_explanations_df.csv\", data_split=[0.7, 0.15, 0.15], tissue_mask_kwargs={\"open\": False, \"close\": False, \"flood\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mittmann/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/utils.py:55: The loaded checkpoint was produced with Lightning v2.2.0.post0, which is newer than your current Lightning version: v2.1.3\n",
      "/home/mittmann/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "model_paths = [Path(f\"GleasonFinal2/label_level1/SoftDiceBalanced-{i}/version_0/checkpoints/best_model.ckpt\") for i in [1, 2, 3]]\n",
    "models = []\n",
    "for model_path in model_paths:\n",
    "    models.append(LitSegmenter.load_from_checkpoint(str(base_path / model_path), map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.augmentations import normalize_only_transform\n",
    "from src.lightning_modul import LitSegmenter\n",
    "from monai.inferers import SlidingWindowInferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output(model, img, device=\"cpu\", label_remapping=None, transform=normalize_only_transform):\n",
    "    model.eval()\n",
    "    inferer = SlidingWindowInferer(roi_size=(512, 512), sw_batch_size=1, overlap=0.5, mode=\"gaussian\")\n",
    "\n",
    "    img = transform(image=img)['image']\n",
    "    \n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = tt.functional.to_tensor(img)\n",
    "    \n",
    "    if len(img.size()) == 3:\n",
    "        no_batch_input = True\n",
    "        img = img.unsqueeze(0)\n",
    "    else:\n",
    "        no_batch_input = False\n",
    "\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if inferer is not None:\n",
    "            out = inferer(img, model)\n",
    "        else:\n",
    "            out = model(img)\n",
    "\n",
    "    # Move back and strip batch_dim\n",
    "    out = out.cpu()\n",
    "\n",
    "    if label_remapping is not None:\n",
    "        out = label_remapping(out)\n",
    "\n",
    "    if no_batch_input:\n",
    "        out = out[0, ...]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    device = 'cpu'\n",
    "    preds_ensemble = generate_model_output(models[0], img)\n",
    "    for model in models[1:]:\n",
    "            out = generate_model_output(model, img, device)\n",
    "            preds_ensemble += out\n",
    "    preds_ensemble = torch.nn.functional.softmax(preds_ensemble, dim=0)\n",
    "    np_seg = np.array(preds_ensemble.argmax(dim=0)).astype(np.uint8)\n",
    "    return np_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relevant classes: 6, 7, 8, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing_5 = 0\n",
    "num_hit_5 = 0\n",
    "num_no5 = 0\n",
    "num_has5 = 0\n",
    "num_overlap5 = 0\n",
    "all_images = 0\n",
    "\n",
    "pixels_hit = 0\n",
    "pixels_total_annotated = 0\n",
    "pixels_total_predicted = 0\n",
    "\n",
    "target_classes = [6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 81/152 [02:42<02:21,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [04:22<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    all_images += 1\n",
    "    img, mask, bg_mask = test_data.get(i, False)\n",
    "    np_masks = np.array(mask)\n",
    "\n",
    "    agreement_count = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=np_masks)\n",
    "    relevant_agreement_counts = agreement_count[target_classes, :]\n",
    "    relevant_pixels = np.any((relevant_agreement_counts == 2) | (relevant_agreement_counts == 3), axis=0) #all pixels in the given class with 2 or 3 annotators\n",
    "    \n",
    "    if not np.any(relevant_pixels):\n",
    "        num_no5 += 1\n",
    "        continue\n",
    "    num_has5 += 1\n",
    "\n",
    "    pixels_total_annotated += np.sum(relevant_pixels)\n",
    "\n",
    "    prediction = predict(img)\n",
    "    patternfive_prediction = np.isin(prediction, [6, 7, 8, 9])\n",
    "    patternfive_prediction = np.where(bg_mask, 0, patternfive_prediction) #remove pixels annotated as 5 in background\n",
    "\n",
    "    if not np.any(patternfive_prediction):\n",
    "        num_missing_5 += 1\n",
    "        print(\"missed\", i)\n",
    "        continue\n",
    "    num_hit_5 += 1\n",
    "    pixels_total_predicted += np.sum(patternfive_prediction)\n",
    "\n",
    "    area_of_interest = np.where(relevant_pixels, patternfive_prediction, False)  # Keep pixels annotation and prediction overlap, else false\n",
    "    pixels_hit += np.sum(area_of_interest)\n",
    "\n",
    "    if np.any(area_of_interest): \n",
    "        num_overlap5 += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [12:16<00:00,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 82\n",
      "yes: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check total amount of GP5 prediction on images\n",
    "no_5_prediction = 0 \n",
    "class_5_prediction = 0\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    all_images += 1\n",
    "    img, mask, bg_mask = test_data.get(i, False)\n",
    "    np_masks = np.array(mask)\n",
    "\n",
    "    prediction = predict(img)\n",
    "    patternfive_prediction = np.isin(prediction, [6, 7, 8, 9])\n",
    "    patternfive_prediction = np.where(bg_mask, 0, patternfive_prediction) #remove pixels annotated as 5 in background\n",
    "\n",
    "    if not np.any(patternfive_prediction):\n",
    "        no_5_prediction += 1\n",
    "        continue\n",
    "    class_5_prediction += 1\n",
    "\n",
    "print(\"no:\", no_5_prediction)\n",
    "print(\"yes:\", class_5_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- FILES --\n",
      "- total files: 152\n",
      "- no 5 anno: 115\n",
      "- has 5 anno: 37\n",
      "- missed: 1\n",
      "- hit: 36\n",
      "- overlap: 34\n",
      "\n",
      "-- PIXELS --\n",
      "- annotated: 6813435\n",
      "- predicted: 7527722\n",
      "- overlap: 5732197\n"
     ]
    }
   ],
   "source": [
    "print(\"-- FILES --\")\n",
    "print(\"- total files:\", all_images)\n",
    "print(\"- no 5 anno:\", num_no5)\n",
    "print(\"- has 5 anno:\", num_has5)\n",
    "print(\"- missed:\", num_missing_5)\n",
    "print(\"- hit:\", num_hit_5)\n",
    "print(\"- overlap:\", num_overlap5)\n",
    "print(\"\")\n",
    "print(\"-- PIXELS --\")\n",
    "print(\"- annotated:\", pixels_total_annotated)\n",
    "print(\"- predicted:\", pixels_total_predicted)\n",
    "print(\"- overlap:\", pixels_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missed image contained annotations for classes 6 (1 annotator) and 7 (two annotators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check equal agreement pixels for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1015 [00:26<1:04:16,  3.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m stacked_arrays \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([mask[\u001b[38;5;241m0\u001b[39m], mask[\u001b[38;5;241m1\u001b[39m], mask[\u001b[38;5;241m2\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Identify positions where all three arrays differ\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m unique_values_per_position \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacked_arrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m all_differ_mask \u001b[38;5;241m=\u001b[39m (unique_values_per_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract values at the differing positions\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/numpy/lib/shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m buff[ind0] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[0;32m--> 402\u001b[0m     buff[ind] \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43mfunc1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minarr_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     buff \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39m__array_wrap__(buff)\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m stacked_arrays \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([mask[\u001b[38;5;241m0\u001b[39m], mask[\u001b[38;5;241m1\u001b[39m], mask[\u001b[38;5;241m2\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Identify positions where all three arrays differ\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m unique_values_per_position \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, arr\u001b[38;5;241m=\u001b[39mstacked_arrays)\n\u001b[1;32m     13\u001b[0m all_differ_mask \u001b[38;5;241m=\u001b[39m (unique_values_per_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract values at the differing positions\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/finalGleasonXAI/lib/python3.10/site-packages/numpy/lib/arraysetops.py:338\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    336\u001b[0m     ar\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[0;32m--> 338\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43maux\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m mask[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (equal_nan \u001b[38;5;129;01mand\u001b[39;00m aux\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m aux\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfmM\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    341\u001b[0m         np\u001b[38;5;241m.\u001b[39misnan(aux[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conf_matrix1_sum = np.zeros((data.num_classes, data.num_classes), dtype=int)\n",
    "conf_matrix2_sum = np.zeros((data.num_classes, data.num_classes), dtype=int)\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    img, mask, background_mask = data.get(i, False)\n",
    "\n",
    "    #np_masks = np.int8(np.array(mask))\n",
    "    #agreement_count = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=np_masks)\n",
    "    stacked_arrays = np.stack([mask[0], mask[1], mask[2]], axis=-1)\n",
    "\n",
    "    # Identify positions where all three arrays differ\n",
    "    unique_values_per_position = np.apply_along_axis(lambda x: len(np.unique(x)), axis=-1, arr=stacked_arrays)\n",
    "    all_differ_mask = (unique_values_per_position == 3)\n",
    "\n",
    "    # Extract values at the differing positions\n",
    "    differing_values = stacked_arrays[all_differ_mask]\n",
    "\n",
    "    # Create confusion matrices for each pair\n",
    "    labels_true = differing_values[:, 0]\n",
    "    labels_pred1 = differing_values[:, 1]\n",
    "    labels_pred2 = differing_values[:, 2]\n",
    "\n",
    "    labels_true = differing_values[:, 0]\n",
    "    labels_pred1 = differing_values[:, 1]\n",
    "    labels_pred2 = differing_values[:, 2]\n",
    "\n",
    "    # Confusion matrices for each pair\n",
    "    conf_matrix1_sum += confusion_matrix(labels_true, labels_pred1, labels=range(10))\n",
    "    conf_matrix2_sum += confusion_matrix(labels_true, labels_pred2, labels=range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [09:58<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "triple_counter = Counter()\n",
    "for i in tqdm(range(len(data))):\n",
    "    img, mask, background_mask = data.get(i, False)\n",
    "\n",
    "    #np_masks = np.int8(np.array(mask))\n",
    "    #agreement_count = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=np_masks)\n",
    "    stacked_arrays = np.stack([mask[0], mask[1], mask[2]], axis=-1)\n",
    "    \n",
    "    triples = stacked_arrays.reshape(-1, 3)\n",
    "    triples = np.sort(triples)\n",
    "    \n",
    "    # Convert triples to tuples and update the counter\n",
    "    triple_counter.update(map(tuple, triples))\n",
    "\n",
    "\n",
    "triple_frequency = dict(triple_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Triples:\n",
      "(0, 3, 4): 12332157\n",
      "(0, 1, 3): 4435248\n",
      "(0, 1, 2): 4199887\n",
      "(3, 6, 9): 3591597\n",
      "(0, 6, 8): 3086826\n",
      "(6, 8, 9): 2810926\n",
      "(3, 4, 5): 2056922\n",
      "(3, 8, 9): 1999913\n",
      "(0, 6, 9): 1936629\n",
      "(0, 3, 6): 1862838\n",
      "(0, 7, 8): 1634772\n",
      "(0, 3, 8): 1621977\n",
      "(6, 7, 8): 1361321\n",
      "(0, 8, 9): 1093395\n",
      "(0, 6, 7): 1068881\n",
      "(0, 3, 5): 1048490\n",
      "(4, 6, 9): 1036751\n",
      "(1, 2, 3): 1016600\n",
      "(3, 7, 9): 843660\n",
      "(6, 7, 9): 813253\n",
      "(4, 6, 7): 760673\n",
      "(3, 4, 6): 602869\n",
      "(1, 3, 4): 561907\n",
      "(0, 1, 4): 532898\n",
      "(3, 7, 8): 457284\n",
      "(3, 6, 8): 453252\n",
      "(0, 2, 3): 437474\n",
      "(3, 6, 7): 434621\n",
      "(0, 4, 5): 432921\n",
      "(3, 4, 9): 363030\n",
      "(0, 3, 7): 270223\n",
      "(0, 4, 7): 268633\n",
      "(0, 3, 9): 231950\n",
      "(0, 4, 9): 224942\n",
      "(1, 2, 4): 197099\n",
      "(0, 7, 9): 161783\n",
      "(0, 2, 4): 159276\n",
      "(4, 8, 9): 159114\n",
      "(0, 4, 6): 154401\n",
      "(3, 4, 8): 142388\n",
      "(0, 1, 5): 122844\n",
      "(1, 3, 5): 107382\n",
      "(0, 4, 8): 89891\n",
      "(3, 4, 7): 75886\n",
      "(1, 8, 9): 66604\n",
      "(7, 8, 9): 58796\n",
      "(1, 4, 5): 58578\n",
      "(1, 7, 8): 50582\n",
      "(1, 2, 5): 49377\n",
      "(1, 6, 9): 46206\n",
      "(2, 3, 4): 42173\n",
      "(0, 2, 5): 41946\n",
      "(1, 6, 7): 36284\n",
      "(2, 4, 5): 18139\n",
      "(1, 6, 8): 16978\n",
      "(1, 3, 9): 13250\n",
      "(0, 1, 8): 12811\n",
      "(0, 1, 7): 9822\n",
      "(3, 5, 8): 7409\n",
      "(1, 3, 7): 6982\n",
      "(2, 3, 5): 6503\n",
      "(0, 5, 8): 4068\n",
      "(2, 3, 8): 3684\n",
      "(4, 6, 8): 3604\n",
      "(0, 1, 6): 3439\n",
      "(1, 4, 9): 3388\n",
      "(1, 3, 8): 3063\n",
      "(2, 4, 9): 2738\n",
      "(4, 7, 8): 1423\n",
      "(0, 5, 6): 1152\n",
      "(3, 5, 6): 617\n",
      "(4, 7, 9): 480\n",
      "(2, 7, 8): 467\n",
      "(2, 4, 8): 329\n",
      "(1, 5, 8): 265\n",
      "(0, 2, 8): 133\n",
      "(0, 1, 9): 62\n"
     ]
    }
   ],
   "source": [
    "unique_triple_frequency = {triple: count for triple, count in triple_frequency.items() if len(set(triple)) == 3}\n",
    "sorted_triple_frequency = dict(sorted(unique_triple_frequency.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(\"Sorted Triples:\")\n",
    "for triple, count in sorted_triple_frequency.items():\n",
    "    print(f\"{triple}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poorly formed and fused glands\n",
      "Cribriform glands\n"
     ]
    }
   ],
   "source": [
    "print(name_dict[3])\n",
    "print(name_dict[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57826136\n",
      "Sorted Triples:\n",
      "(0, 3, 4): 0.21326268454112168\n",
      "(0, 1, 3): 0.07669971239302588\n",
      "(0, 1, 2): 0.07262956321342308\n",
      "(3, 6, 9): 0.06211027138316833\n",
      "(0, 6, 8): 0.05338115623011712\n",
      "(6, 8, 9): 0.048609957269149025\n",
      "(3, 4, 5): 0.03557080141062858\n",
      "(3, 8, 9): 0.03458493232195214\n",
      "(0, 6, 9): 0.03349054828771544\n",
      "(0, 3, 6): 0.032214464407582064\n",
      "(0, 7, 8): 0.02827046925632382\n",
      "(0, 3, 8): 0.02804920252669139\n",
      "(6, 7, 8): 0.023541621387256448\n",
      "(0, 8, 9): 0.01890831854993735\n",
      "(0, 6, 7): 0.018484392593688087\n",
      "(0, 3, 5): 0.018131766576967894\n",
      "(4, 6, 9): 0.017928761485982738\n",
      "(1, 2, 3): 0.017580285841682385\n",
      "(3, 7, 9): 0.014589596648823294\n",
      "(6, 7, 9): 0.014063761756448676\n",
      "(4, 6, 7): 0.013154484332136597\n",
      "(3, 4, 6): 0.010425545293221736\n",
      "(1, 3, 4): 0.00971718048046648\n",
      "(0, 1, 4): 0.009215521507437398\n",
      "(3, 7, 8): 0.0079079120901317\n",
      "(3, 6, 8): 0.007838185833478482\n",
      "(0, 2, 3): 0.007565333433311194\n",
      "(3, 6, 7): 0.00751599588117041\n",
      "(0, 4, 5): 0.0074865974098632495\n",
      "(3, 4, 9): 0.006277957081552189\n",
      "(0, 3, 7): 0.004673025360020597\n",
      "(0, 4, 7): 0.0046455291427391935\n",
      "(0, 3, 9): 0.004011162011585903\n",
      "(0, 4, 9): 0.003889971136926735\n",
      "(1, 2, 4): 0.0034084760565706827\n",
      "(0, 7, 9): 0.0027977487549920334\n",
      "(0, 2, 4): 0.0027543946564231787\n",
      "(4, 8, 9): 0.00275159315503979\n",
      "(0, 4, 6): 0.0026700902166452898\n",
      "(3, 4, 8): 0.002462346783814156\n",
      "(0, 1, 5): 0.0021243681230922987\n",
      "(1, 3, 5): 0.0018569803799444596\n",
      "(0, 4, 8): 0.0015545046966306032\n",
      "(3, 4, 7): 0.001312313172714843\n",
      "(1, 8, 9): 0.001151797519377743\n",
      "(7, 8, 9): 0.0010167720699857932\n",
      "(1, 4, 5): 0.0010130021483711101\n",
      "(1, 7, 8): 0.0008747255739169568\n",
      "(1, 2, 5): 0.0008538872457257043\n",
      "(1, 6, 9): 0.0007990504501286408\n",
      "(2, 3, 4): 0.0007293069002570049\n",
      "(0, 2, 5): 0.0007253813396765781\n",
      "(1, 6, 7): 0.0006274671370053153\n",
      "(2, 4, 5): 0.0003136816888474098\n",
      "(1, 6, 8): 0.0002936042622664603\n",
      "(1, 3, 9): 0.00022913514401169742\n",
      "(0, 1, 8): 0.00022154342112708344\n",
      "(0, 1, 7): 0.00016985399128172768\n",
      "(3, 5, 8): 0.0001281254552439748\n",
      "(1, 3, 7): 0.00012074125098035255\n",
      "(2, 3, 5): 0.00011245779935909949\n",
      "(0, 5, 8): 7.03488125161951e-05\n",
      "(2, 3, 8): 6.370821664445987e-05\n",
      "(4, 6, 8): 6.23247591711817e-05\n",
      "(0, 1, 6): 5.9471378132545465e-05\n",
      "(1, 4, 9): 5.858942399333063e-05\n",
      "(1, 3, 8): 5.296912800813805e-05\n",
      "(2, 4, 9): 4.734883202294547e-05\n",
      "(4, 7, 8): 2.46082498059355e-05\n",
      "(0, 5, 6): 1.9921787615205692e-05\n",
      "(3, 5, 6): 1.066991576265791e-05\n",
      "(4, 7, 9): 8.300744839669038e-06\n",
      "(2, 7, 8): 8.075933000261335e-06\n",
      "(2, 4, 8): 5.6894688588564865e-06\n",
      "(1, 5, 8): 4.5827028802339484e-06\n",
      "(0, 2, 8): 2.2999980493249625e-06\n",
      "(0, 1, 9): 1.072179541790584e-06\n"
     ]
    }
   ],
   "source": [
    "total_pixels = 0\n",
    "for val in sorted_triple_frequency.values():\n",
    "    total_pixels += val\n",
    "print(total_pixels)\n",
    "\n",
    "print(\"Sorted Triples:\")\n",
    "for triple, count in sorted_triple_frequency.items():\n",
    "    print(f\"{triple}: {count / total_pixels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Triple Frequencies (containing 3 and 4):\n",
      "(0, 3, 4)\n",
      "(3, 4, 5)\n",
      "(3, 4, 6)\n",
      "(1, 3, 4)\n",
      "(3, 4, 9)\n",
      "(3, 4, 8)\n",
      "(3, 4, 7)\n",
      "(2, 3, 4)\n",
      "0.27975813566377666\n"
     ]
    }
   ],
   "source": [
    "# check 3 and 4\n",
    "filtered_triple_frequency = {\n",
    "    triple: count for triple, count in sorted_triple_frequency.items() if 3 in triple and 4 in triple\n",
    "}\n",
    "\n",
    "# Display the filtered results\n",
    "sum_with_3_4 = 0\n",
    "print(\"Filtered Triple Frequencies (containing 3 and 4):\")\n",
    "for triple, count in filtered_triple_frequency.items():\n",
    "    print(triple)\n",
    "    sum_with_3_4 += count\n",
    "print(sum_with_3_4 / total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Triple Frequencies (background):\n",
      "0.6481804179342019\n"
     ]
    }
   ],
   "source": [
    "# check 0\n",
    "filtered_triple_frequency = {\n",
    "    triple: count for triple, count in sorted_triple_frequency.items() if 0 in triple \n",
    "}\n",
    "\n",
    "# Display the filtered results\n",
    "sum_with_0 = 0\n",
    "print(\"Filtered Triple Frequencies (background):\")\n",
    "for triple, count in filtered_triple_frequency.items():\n",
    "    sum_with_0 += count\n",
    "print(sum_with_0 / total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with mapping to grade\n",
    "def map_values(value):\n",
    "    if value in [1, 2]:\n",
    "        return 3\n",
    "    elif value in [3, 4, 5]:\n",
    "        return 4\n",
    "    elif value in [6, 7, 8, 9]:\n",
    "        return 5\n",
    "    return value\n",
    "\n",
    "summed_triple_frequency = defaultdict(int)\n",
    "\n",
    "for triple, count in sorted_triple_frequency.items():\n",
    "    mapped_triple = tuple(map(map_values, triple))\n",
    "    summed_triple_frequency[mapped_triple] += count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Triple Frequencies (in grades):\n",
      "(0, 4, 4): 13813568\n",
      "(0, 3, 4): 5729686\n",
      "(0, 3, 3): 4199887\n",
      "(4, 5, 5): 9742372\n",
      "(0, 5, 5): 8982286\n",
      "(5, 5, 5): 5044296\n",
      "(4, 4, 4): 2056922\n",
      "(0, 4, 5): 4730075\n",
      "(3, 3, 4): 1263076\n",
      "(4, 4, 5): 1192199\n",
      "(3, 4, 4): 794682\n",
      "(3, 5, 5): 217121\n",
      "(3, 4, 5): 33699\n",
      "(0, 3, 5): 26267\n",
      "57826136\n"
     ]
    }
   ],
   "source": [
    "summed_triple_frequency = dict(summed_triple_frequency)\n",
    "total_pixels = 0\n",
    "print(\"Filtered Triple Frequencies (in grades):\")\n",
    "for triple, count in summed_triple_frequency.items():\n",
    "    total_pixels += count\n",
    "    print(f\"{triple}: {count}\")\n",
    "print(total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 4, 4): 0.23888104852795283\n",
      "(0, 3, 4): 0.09908471145296653\n",
      "(0, 3, 3): 0.07262956321342308\n",
      "(4, 5, 5): 0.16847696688570027\n",
      "(0, 5, 5): 0.15533263367277383\n",
      "(5, 5, 5): 0.08723211248283994\n",
      "(4, 4, 4): 0.03557080141062858\n",
      "(0, 4, 5): 0.08179822009895318\n",
      "(3, 3, 4): 0.021842649143978772\n",
      "(4, 4, 5): 0.02061695770230956\n",
      "(3, 4, 4): 0.013742609397245564\n",
      "(3, 5, 5): 0.0037547208756953778\n",
      "(3, 4, 5): 0.0005827641674000144\n",
      "(0, 3, 5): 0.00045424096813247215\n"
     ]
    }
   ],
   "source": [
    "for triple, count in summed_triple_frequency.items():\n",
    "    print(f\"{triple}: {count / total_pixels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004791726011227865\n"
     ]
    }
   ],
   "source": [
    "# contain 3 and 5:\n",
    "contain_3_and_5 = 0.0037547208756953778 + 0.0005827641674000144 + 0.00045424096813247215\n",
    "print(contain_3_and_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 and 3: 0.09447221235740184\n",
      "4 and 4: 0.3088114170381365\n",
      "5 and 5: 0.41479643391700943\n",
      "\n",
      "3 and 4: 0.13525273416159087\n",
      "3 and 5: 0.004791726011227864\n",
      "4 and 5: 0.27147490885436304\n",
      "\n",
      "all 4: 0.03557080141062858\n",
      "all 5: 0.08723211248283994\n"
     ]
    }
   ],
   "source": [
    "sum_3_and_3 = 0\n",
    "sum_3_and_4 = 0\n",
    "sum_3_and_5 = 0\n",
    "sum_4_and_4 = 0\n",
    "sum_4_and_5 = 0\n",
    "sum_5_and_5 = 0\n",
    "\n",
    "# Check each triple in the summed dictionary\n",
    "for triple, count in summed_triple_frequency.items():\n",
    "    if 3 in triple and 4 in triple:\n",
    "        sum_3_and_4 += count\n",
    "    if 3 in triple and 5 in triple:\n",
    "        sum_3_and_5 += count\n",
    "    if 4 in triple and 5 in triple:\n",
    "        sum_4_and_5 += count\n",
    "    if triple.count(3) >= 2:  \n",
    "        sum_3_and_3 += count\n",
    "    if triple.count(4) >= 2:  \n",
    "        sum_4_and_4 += count\n",
    "    if triple.count(5) >= 2:  \n",
    "        sum_5_and_5 += count\n",
    "\n",
    "print(f\"3 and 3:\", sum_3_and_3 / total_pixels)\n",
    "print(f\"4 and 4:\", sum_4_and_4 / total_pixels)\n",
    "print(f\"5 and 5:\", sum_5_and_5 / total_pixels)\n",
    "print(\"\")\n",
    "print(f\"3 and 4:\", sum_3_and_4 / total_pixels)\n",
    "print(f\"3 and 5:\", sum_3_and_5 / total_pixels)\n",
    "print(f\"4 and 5:\", sum_4_and_5 / total_pixels)\n",
    "print(\"\")\n",
    "#print(\"all 3:\", summed_triple_frequency[(3, 3, 3)])\n",
    "print(\"all 4:\", summed_triple_frequency[(4, 4, 4)] / total_pixels)\n",
    "print(\"all 5:\", summed_triple_frequency[(5, 5, 5)] / total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all different: 0.18191993668745218\n",
      "all equal: 0.12280291389346852\n"
     ]
    }
   ],
   "source": [
    "sum_all_equal = 0\n",
    "sum_all_different = 0\n",
    "\n",
    "for triple, count in summed_triple_frequency.items():\n",
    "    num_grades = len(set(triple))\n",
    "    if num_grades == 3:\n",
    "        sum_all_different += count\n",
    "    elif num_grades == 1:\n",
    "        sum_all_equal += count\n",
    "\n",
    "print(\"all different:\", sum_all_different / total_pixels)\n",
    "print(\"all equal:\", sum_all_equal / total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- of remaining pixels:\n",
      "3: 0.13587705627365382\n",
      "4: 0.3929952478027029\n",
      "5: 0.4711276959236433\n",
      "\n",
      "- of total pixels:\n",
      "3: 0.09447221235740184\n",
      "4: 0.27324061562750795\n",
      "5: 0.32756432143416947\n"
     ]
    }
   ],
   "source": [
    "sum_two_3 = 0\n",
    "sum_two_4 = 0\n",
    "sum_two_5 = 0\n",
    "\n",
    "for triple, count in summed_triple_frequency.items():\n",
    "    num_grades = len(set(triple))\n",
    "    if num_grades == 2:\n",
    "        if triple.count(3) >= 2:  \n",
    "            sum_two_3 += count\n",
    "        if triple.count(4) >= 2:  \n",
    "            sum_two_4 += count\n",
    "        if triple.count(5) >= 2:  \n",
    "            sum_two_5 += count\n",
    "\n",
    "remaining_pixels = total_pixels - sum_all_different - sum_all_equal\n",
    "print(\"- of remaining pixels:\")\n",
    "print(\"3:\", sum_two_3 / remaining_pixels)\n",
    "print(\"4:\", sum_two_4 / remaining_pixels)\n",
    "print(\"5:\", sum_two_5 / remaining_pixels)\n",
    "print(\"\")\n",
    "print(\"- of total pixels:\")\n",
    "print(\"3:\", sum_two_3 / total_pixels)\n",
    "print(\"4:\", sum_two_4 / total_pixels)\n",
    "print(\"5:\", sum_two_5 / total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP 5 with:\n",
      "- 0: 0.4742049835973696\n",
      "- 3: 0.01146254530791432\n",
      "- 4: 0.514332471094716\n"
     ]
    }
   ],
   "source": [
    "# of the triples with 2 times GP5 explanations, which third class is most often:\n",
    "five_with_0 = 0\n",
    "five_with_3 = 0\n",
    "five_with_4 = 0\n",
    "total_5 = 0\n",
    "\n",
    "for triple, count in summed_triple_frequency.items():\n",
    "    num_grades = len(set(triple))\n",
    "    if num_grades == 2:\n",
    "        if triple.count(5) == 2: \n",
    "            total_5 += count\n",
    "            if 0 in triple:\n",
    "                five_with_0 += count\n",
    "            elif 3 in triple:\n",
    "                five_with_3 += count\n",
    "            elif 4 in triple:\n",
    "                five_with_4 += count\n",
    "\n",
    "print(\"GP 5 with:\")\n",
    "print(\"- 0:\", five_with_0 / total_5)\n",
    "print(\"- 3:\", five_with_3 / total_5)\n",
    "print(\"- 4:\", five_with_4 / total_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP 4 with:\n",
      "- 0: 0.8742516114573706\n",
      "- 3: 0.05029489984746636\n",
      "- 4: 0.07545348869516302\n"
     ]
    }
   ],
   "source": [
    "# of the triples with 2 times GP4 explanations, which third class is most often:\n",
    "four_with_0 = 0\n",
    "four_with_3 = 0\n",
    "four_with_5 = 0\n",
    "total_4 = 0\n",
    "\n",
    "for triple, count in summed_triple_frequency.items():\n",
    "    num_grades = len(set(triple))\n",
    "    if num_grades == 2:\n",
    "        if triple.count(4) == 2: \n",
    "            total_4 += count\n",
    "            if 0 in triple:\n",
    "                four_with_0 += count\n",
    "            elif 3 in triple:\n",
    "                four_with_3 += count\n",
    "            elif 4 in triple:\n",
    "                four_with_5 += count\n",
    "\n",
    "print(\"GP 4 with:\")\n",
    "print(\"- 0:\", four_with_0 / total_4)\n",
    "print(\"- 3:\", four_with_3 / total_4)\n",
    "print(\"- 4:\", four_with_5 / total_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP 3 with:\n",
      "- 0: 0.7687928693641162\n",
      "- 4: 0.23120713063588386\n",
      "- 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "# of the triples with 2 times GP3 explanations, which third class is most often:\n",
    "three_with_0 = 0\n",
    "three_with_4 = 0\n",
    "three_with_5 = 0\n",
    "total_3 = 0\n",
    "\n",
    "for triple, count in summed_triple_frequency.items():\n",
    "    num_grades = len(set(triple))\n",
    "    if num_grades == 2:\n",
    "        if triple.count(3) == 2: \n",
    "            total_3 += count\n",
    "            if 0 in triple:\n",
    "                three_with_0 += count\n",
    "            elif 4 in triple:\n",
    "                three_with_4 += count\n",
    "            elif 5 in triple:\n",
    "                three_with_5 += count\n",
    "\n",
    "print(\"GP 3 with:\")\n",
    "print(\"- 0:\", three_with_0 / total_3)\n",
    "print(\"- 4:\", three_with_4 / total_3)\n",
    "print(\"- 5:\", three_with_5 / total_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check with background removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [09:51<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "triple_counter = Counter()\n",
    "for i in tqdm(range(len(data))):\n",
    "    img, mask, background_mask = data.get(i, False)\n",
    "    \n",
    "    cleaned_masks = []\n",
    "    for current_mask in mask:\n",
    "        current_mask = current_mask + 1\n",
    "        current_mask = np.where(~background_mask, current_mask, 0)\n",
    "        cleaned_masks.append(current_mask)\n",
    "\n",
    "    #np_masks = np.int8(np.array(mask))\n",
    "    #agreement_count = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=np_masks)\n",
    "    stacked_arrays = np.stack([cleaned_masks[0], cleaned_masks[1], cleaned_masks[2]], axis=-1)\n",
    "    \n",
    "    triples = stacked_arrays.reshape(-1, 3)\n",
    "    triples -= 1\n",
    "    triples = np.sort(triples)\n",
    "    \n",
    "    # Convert triples to tuples and update the counter\n",
    "    triple_counter.update(map(tuple, triples))\n",
    "\n",
    "\n",
    "triple_frequency = dict(triple_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52218501\n",
      "Sorted Triples:\n",
      "(0, 3, 4): 0.20440496750375886\n",
      "(0, 1, 3): 0.07415007949002596\n",
      "(0, 1, 2): 0.06844484103440655\n",
      "(3, 6, 9): 0.0674513234303681\n",
      "(0, 6, 8): 0.05551099599737649\n",
      "(6, 8, 9): 0.050929535491645\n",
      "(3, 8, 9): 0.03699811298681285\n",
      "(0, 3, 6): 0.03399425425865825\n",
      "(0, 6, 9): 0.033154743373426214\n",
      "(3, 4, 5): 0.032467190124818024\n",
      "(0, 3, 8): 0.03007786454842892\n",
      "(0, 7, 8): 0.029784443640004144\n",
      "(6, 7, 8): 0.025658645390835712\n",
      "(0, 8, 9): 0.019269683746762474\n",
      "(4, 6, 9): 0.018998975095052998\n",
      "(0, 6, 7): 0.01899234909098597\n",
      "(0, 3, 5): 0.018010723823726767\n",
      "(1, 2, 3): 0.01666547264541355\n",
      "(3, 7, 9): 0.015731857182189123\n",
      "(6, 7, 9): 0.014099542995307354\n",
      "(4, 6, 7): 0.01158710013525666\n",
      "(3, 4, 6): 0.010902879038982755\n",
      "(0, 1, 4): 0.00928781927309633\n",
      "(1, 3, 4): 0.009166827672820406\n",
      "(3, 6, 8): 0.008199775784448504\n",
      "(3, 7, 8): 0.008046362724966004\n",
      "(3, 6, 7): 0.007998161417923505\n",
      "(0, 4, 5): 0.007332707616405917\n",
      "(0, 2, 3): 0.007029673256993723\n",
      "(3, 4, 9): 0.006249241049642539\n",
      "(0, 4, 7): 0.005013031683923673\n",
      "(0, 3, 7): 0.004904698432457876\n",
      "(0, 3, 9): 0.004049177895780655\n",
      "(1, 2, 4): 0.003237511547870744\n",
      "(0, 7, 9): 0.0028798605306575155\n",
      "(4, 8, 9): 0.002720893883951207\n",
      "(0, 4, 6): 0.0026291256426529746\n",
      "(0, 2, 4): 0.002553922411522307\n",
      "(3, 4, 8): 0.002543619549707105\n",
      "(0, 1, 5): 0.002151555442007039\n",
      "(0, 4, 9): 0.0019747407149814586\n",
      "(1, 3, 5): 0.0017720539316132418\n",
      "(0, 4, 8): 0.0016297671968791292\n",
      "(3, 4, 7): 0.0014026637800269295\n",
      "(1, 8, 9): 0.0011990577822216688\n",
      "(7, 8, 9): 0.0011250035691373064\n",
      "(1, 4, 5): 0.0009346687297668694\n",
      "(1, 7, 8): 0.0009285980844222242\n",
      "(1, 6, 9): 0.0008728707091764277\n",
      "(1, 2, 5): 0.0007534111329622426\n",
      "(2, 3, 4): 0.000740905986558289\n",
      "(0, 2, 5): 0.0006981433649349681\n",
      "(1, 6, 7): 0.0006855616173279275\n",
      "(1, 6, 8): 0.0002620335654598741\n",
      "(1, 3, 9): 0.000253645733721847\n",
      "(0, 1, 8): 0.00022775452707843912\n",
      "(2, 4, 5): 0.00021572813819377925\n",
      "(0, 1, 7): 0.00016298820986837597\n",
      "(3, 5, 8): 0.00013657994510413081\n",
      "(1, 3, 7): 0.00011553376455597604\n",
      "(2, 3, 5): 0.0001044074398075885\n",
      "(0, 5, 8): 7.619904677079872e-05\n",
      "(2, 3, 8): 7.054970804313206e-05\n",
      "(0, 1, 6): 6.229592841050723e-05\n",
      "(1, 3, 8): 5.863822096310271e-05\n",
      "(1, 4, 9): 5.624443336663379e-05\n",
      "(4, 6, 8): 5.385064577016487e-05\n",
      "(2, 4, 9): 5.228032110688126e-05\n",
      "(4, 7, 8): 2.7250877998202207e-05\n",
      "(0, 5, 6): 2.2061146489057586e-05\n",
      "(3, 5, 6): 1.18157355761706e-05\n",
      "(4, 7, 9): 9.19214437044066e-06\n",
      "(2, 7, 8): 8.943190460407893e-06\n",
      "(2, 4, 8): 6.300448953906202e-06\n",
      "(1, 5, 8): 4.979078200655358e-06\n",
      "(0, 2, 8): 2.546990002642933e-06\n",
      "(0, 1, 9): 1.1873186478485854e-06\n"
     ]
    }
   ],
   "source": [
    "unique_triple_frequency = {triple: count for triple, count in triple_frequency.items() if len(set(triple)) == 3}\n",
    "sorted_triple_frequency = dict(sorted(unique_triple_frequency.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "total_pixels = 0\n",
    "for val in sorted_triple_frequency.values():\n",
    "    total_pixels += val\n",
    "print(total_pixels)\n",
    "\n",
    "print(\"Sorted Triples:\")\n",
    "for triple, count in sorted_triple_frequency.items():\n",
    "    print(f\"{triple}: {count / total_pixels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Triple Frequencies (containing 3 and 4):\n",
      "(0, 3, 4)\n",
      "(3, 4, 5)\n",
      "(3, 4, 6)\n",
      "(1, 3, 4)\n",
      "(3, 4, 9)\n",
      "(3, 4, 8)\n",
      "(3, 4, 7)\n",
      "(2, 3, 4)\n",
      "0.2678782947063149\n"
     ]
    }
   ],
   "source": [
    "# check 3 and 4\n",
    "filtered_triple_frequency = {\n",
    "    triple: count for triple, count in sorted_triple_frequency.items() if 3 in triple and 4 in triple\n",
    "}\n",
    "\n",
    "# Display the filtered results\n",
    "sum_with_3_4 = 0\n",
    "print(\"Filtered Triple Frequencies (containing 3 and 4):\")\n",
    "for triple, count in filtered_triple_frequency.items():\n",
    "    print(triple)\n",
    "    sum_with_3_4 += count\n",
    "print(sum_with_3_4 / total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Triple Frequencies (containing 3 and 4):\n",
      "0.6384842031371218\n"
     ]
    }
   ],
   "source": [
    "# check 0\n",
    "filtered_triple_frequency = {\n",
    "    triple: count for triple, count in sorted_triple_frequency.items() if 0 in triple \n",
    "}\n",
    "\n",
    "# Display the filtered results\n",
    "sum_with_0 = 0\n",
    "print(\"Filtered Triple Frequencies (containing 3 and 4):\")\n",
    "for triple, count in filtered_triple_frequency.items():\n",
    "    sum_with_0 += count\n",
    "print(sum_with_0 / total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Benign',\n",
       " 1: 'variable sized well-formed individual and discrete glands',\n",
       " 2: 'compressed or angular discrete glands',\n",
       " 3: 'poorly formed and fused glands',\n",
       " 4: 'Cribriform glands',\n",
       " 5: 'Glomeruloid glands',\n",
       " 6: 'solid groups of tumor cells',\n",
       " 7: 'single cells',\n",
       " 8: 'cords',\n",
       " 9: 'presence of comedonecrosis'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalGleasonXAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
