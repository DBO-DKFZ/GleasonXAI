{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jdt_losses import SoftCorrectDICEMetric\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from src.gleason_data import GleasonX\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from torchmetrics import Dice\n",
    "\n",
    "%load_ext autoreload\n",
    "data_path = Path(os.environ[\"DATASET_LOCATION\"] / \"GleasonXAI\")\n",
    "assert data_path.exists()\n",
    "from src.augmentations import basic_transforms_val_test_colorpreserving, normalize_only_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = GleasonX(data_path, split=\"test\", scaling=\"MicronsCalibrated\", transforms=normalize_only_transform, label_level=1, create_seg_masks=True,\n",
    "                     tissue_mask_kwargs={\"open\": False, \"close\": False, \"flood\": False}, drawing_order=\"grade_frame_order\", explanation_file=\"final_filtered_explanations_df.csv\", data_split=(0.7, 0.15, 0.15))\n",
    "data_train = GleasonX(data_path, split=\"train\", scaling=\"MicronsCalibrated\", transforms=normalize_only_transform, label_level=1, create_seg_masks=True,\n",
    "                      tissue_mask_kwargs={\"open\": False, \"close\": False, \"flood\": False}, drawing_order=\"grade_frame_order\", explanation_file=\"final_filtered_explanations_df.csv\", data_split=(0.7, 0.15, 0.15))\n",
    "data_val = GleasonX(data_path, split=\"val\", scaling=\"MicronsCalibrated\", transforms=normalize_only_transform, label_level=1, create_seg_masks=True,\n",
    "                    tissue_mask_kwargs={\"open\": False, \"close\": False, \"flood\": False}, drawing_order=\"grade_frame_order\", explanation_file=\"final_filtered_explanations_df.csv\", data_split=(0.7, 0.15, 0.15))\n",
    "\n",
    "data_all = GleasonX(data_path, split=\"all\", scaling=\"MicronsCalibrated\", transforms=normalize_only_transform, label_level=1, create_seg_masks=True,\n",
    "                    tissue_mask_kwargs={\"open\": False, \"close\": False, \"flood\": False}, drawing_order=\"grade_frame_order\", explanation_file=\"final_filtered_explanations_df.csv\", data_split=(0.7, 0.15, 0.15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of explanations per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(data_all.train_slides)\n",
    "val = list(data_all.val_slides)\n",
    "test = list(data_all.test_slides)\n",
    "\n",
    "\n",
    "grouped = data_all.df[data_all.df[\"TMA_identifier\"].isin(test)].groupby([\"TMA_identifier\", \"explanations\"]).size() > 0\n",
    "grouped.groupby([\"explanations\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute new dataset seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_label_counts_per_slide(data):\n",
    "\n",
    "    num_classes = data[0][1].shape[0]\n",
    "\n",
    "    labels_all = []\n",
    "    labels_fg_only = []\n",
    "    labels_unique_max_only = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        _, label, background = data[i]\n",
    "\n",
    "        label_max = torch.max(label, dim=0)[0]\n",
    "        duplicated_max = torch.sum(label == label_max.unsqueeze(0), dim=0) > 1\n",
    "        unique_max = ~duplicated_max\n",
    "\n",
    "        labels_all.append(label.sum(dim=(1, 2)))\n",
    "\n",
    "        labels_fg_only.append(torch.sum(label[:, ~background].sum(dim=1)))\n",
    "        labels_unique_max_only.append(torch.sum(label[:, torch.logical_and(~background, unique_max)].sum(dim=1)))\n",
    "    return labels_all, labels_fg_only, labels_unique_max_only\n",
    "\n",
    "labels, labels_fg, labels_unique_max = get_label_counts_per_slide(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "num_trials = 100000\n",
    "slides = torch.arange(len(labels))\n",
    "split = (0.7, 0.15, 0.15)\n",
    "best_loss = 1000\n",
    "best_seed = -1\n",
    "for i in tqdm(range(100000)):\n",
    "    train, val, test = random_split(\n",
    "        slides, split, torch.Generator().manual_seed(i))\n",
    "\n",
    "\n",
    "    train = torch.stack(list(train))\n",
    "    val = torch.stack(list(val))\n",
    "    test = torch.stack(list(test))\n",
    "\n",
    "    def dist_comp(idcs):\n",
    "        t =  torch.stack([labels[i] for i in idcs]).sum(dim=0)\n",
    "        return t/t.sum()\n",
    "\n",
    "    train_dist = dist_comp(train)[1:]\n",
    "    val_dist = dist_comp(val)[1:]\n",
    "    test_dist = dist_comp(test)[1:]\n",
    "\n",
    "    def l1(a,b): return ((a-b).abs()/2).sum()\n",
    "\n",
    "    loss = l1(train_dist, val_dist)+l1(val_dist, test_dist)+l1(train_dist, test_dist)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        print(loss, i)\n",
    "        best_loss = loss\n",
    "        best_seed = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_label_counts(data):\n",
    "\n",
    "    num_classes = data[0][1].shape[0]\n",
    "\n",
    "    labels_all = torch.zeros(num_classes)\n",
    "    labels_fg_only = torch.zeros(num_classes)\n",
    "    labels_unique_max_only = torch.zeros(num_classes)\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        _, label, background = data[i]\n",
    "\n",
    "\n",
    "        label_max = torch.max(label, dim=0)[0]\n",
    "        duplicated_max = torch.sum(label == label_max.unsqueeze(0), dim=0) > 1\n",
    "        unique_max = ~duplicated_max\n",
    "        \n",
    "\n",
    "        labels_all += label.sum(dim=(1,2))\n",
    "\n",
    "        labels_fg_only += torch.sum(label[:,~background].sum(dim=1))\n",
    "        labels_unique_max_only += torch.sum(label[:, torch.logical_and(~background, unique_max)].sum(dim=1))\n",
    "    return labels_all, labels_fg_only, labels_unique_max_only\n",
    "\n",
    "\n",
    "\n",
    "labels_train, label_fg_only_train, labels_unique_max_only_train = get_label_counts(data_train)\n",
    "labels_val, label_fg_only_val, labels_unique_max_only_val = get_label_counts(data_val)\n",
    "labels_test, label_fg_only_test, labels_unique_max_only_test = get_label_counts(data_test)\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "def get_freq_max(labels, bgs, unique_maxs):\n",
    "\n",
    "    label_freq = labels.sum(dim=(0, 2, 3))\n",
    "    label_freq /= label_freq.sum()\n",
    "\n",
    "    max_freq = torch.zeros_like(label_freq)\n",
    "    for label, bg, um in zip(labels, bgs, unique_maxs):\n",
    "        fg_mask = ~bg\n",
    "        unique_forground = torch.logical_and(fg_mask, um)\n",
    "        max_freq += torch.bincount(label[:, unique_forground].argmax(dim=0).reshape(-1), minlength=num_classes)\n",
    "\n",
    "    return label_freq, max_freq\n",
    "\n",
    "\n",
    "#label_freq_train, max_freq_train = get_freq_max(labels_train, bgs_train, unique_max_train)\n",
    "#label_freq_val, max_freq_val = get_freq_max(labels_val, bgs_val, unique_max_val)\n",
    "#label_freq_test, max_freq_test = get_freq_max(labels_test, bgs_test, unique_max_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_named = data_train.classes_named\n",
    "bar_width = 0.1\n",
    "plt.bar(np.arange(num_classes)-2.5*bar_width, labels_train/labels_train.sum(), label=\"Train\", width=bar_width)\n",
    "# plt.bar(np.arange(num_classes)-1.5*bar_width, max_freq/torch.sum(max_freq), label=\"Current Anno.: Majority Vote\", width=bar_width)\n",
    "\n",
    "plt.bar(np.arange(num_classes)-0.5*bar_width, labels_val/labels_val.sum(), label=\"Val\", width=bar_width)\n",
    "# plt.bar(np.arange(num_classes)+0.5*bar_width, max_freq2/torch.sum(max_freq2), label=\"Custom Order Anno.: Majority Vote\", width=bar_width)\n",
    "\n",
    "plt.bar(np.arange(num_classes)+1.5*bar_width, labels_test/labels_test.sum(), label=\"Test\", width=bar_width)\n",
    "# plt.bar(np.arange(num_classes)+2.5*bar_width, max_freq3/torch.sum(max_freq3), label=\"Ordered Anno.: Majority Vote\", width=bar_width)\n",
    "\n",
    "\n",
    "# _ = plt.yscale(\"log\")\n",
    "_ = plt.legend()\n",
    "_ = plt.xticks(np.arange(num_classes), list(map(lambda x: x[:20], classes_named)), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "_ = plt.xlabel(\"Grouped Explanation\")\n",
    "_ = plt.ylabel(\"Proportion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare multiple drawing orders\n",
    "\n",
    "This was used when discussing which drawing order we should use. \n",
    "The outputs will not be the same as before, as we changed other things (new dataframe, MicronsCalibrated resizing and the new seed from above etc.)\n",
    "\n",
    "## IMPORTANT: This will crash the cells, as I throw an error in the Gleason data class if you take a drawing_order other than \"grade_frame_order\", as this is the setting we settled on. You will need to change that if you want to rerun this for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_labels_bg(data, drawing_order=None):\n",
    "    labels = []\n",
    "    bgs = []\n",
    "    unique_maxs = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        _, label, background = data.__getitem__(i, drawing_order=drawing_order)\n",
    "        labels.append(label)\n",
    "        bgs.append(background)\n",
    "\n",
    "        label_max = torch.max(label, dim=0)[0]\n",
    "        duplicated_max = torch.sum(label == label_max.unsqueeze(0), dim=0) > 1\n",
    "        unique_max = ~duplicated_max\n",
    "        unique_maxs.append(unique_max)\n",
    "\n",
    "    return labels, bgs, unique_maxs\n",
    "\n",
    "# IMPORTANT: This is older. Back then I used the old data to make these comparisons. It does not matter to much if you just want to visualize them.\n",
    "data = GleasonX(data_path, split=\"test\", scaling=\"MicronsCalibrated\", transforms=normalize_only_transform, label_level=1, create_seg_masks=True,\n",
    "                    tissue_mask_kwargs={\"open\": False, \"close\": False, \"flood\": False}, drawing_order=\"grade_frame_order\", explanation_file=\"final_filtered_explanations_df.csv\", data_split=(0.7, 0.15, 0.15))\n",
    "\n",
    "labels, bgs, unique_maxs = get_labels_bg(data)\n",
    "labels2, bgs2, unique_maxs2 = get_labels_bg(data, \"custom_order\")\n",
    "labels3, bgs3, unique_maxs3 = get_labels_bg(data, \"frame_order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if other drawing orders not available\n",
    "labels2 = labels\n",
    "labels3 = labels\n",
    "bgs2 = bgs\n",
    "bgs3 = bgs\n",
    "unique_maxs2 = unique_maxs\n",
    "unique_maxs3 = unique_maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_classes=10\n",
    "def get_freq_max(labels, bgs, unique_maxs):\n",
    "    \n",
    "    label_freq = torch.sum(torch.stack([label.sum(dim=(1, 2)) for label in labels]), dim=0)  # labels.sum(dim=(0, 2, 3)) but we dont a same sized first dimension, instead we have a list.\n",
    "    label_freq /= label_freq.sum()\n",
    "\n",
    "    max_freq = torch.zeros_like(label_freq)\n",
    "    for label,bg,um in zip(labels, bgs, unique_maxs):\n",
    "        fg_mask = ~bg\n",
    "        unique_forground = torch.logical_and(fg_mask, um)\n",
    "        max_freq+= torch.bincount(label[:, unique_forground].argmax(dim=0).reshape(-1),minlength=num_classes)\n",
    "\n",
    "    return label_freq, max_freq\n",
    "\n",
    "\n",
    "label_freq, max_freq = get_freq_max(labels, bgs, unique_maxs)\n",
    "label_freq2, max_freq2 = get_freq_max(labels2, bgs2, unique_maxs2)\n",
    "label_freq3, max_freq3 = get_freq_max(labels3, bgs3, unique_maxs3)\n",
    "\n",
    "classes_named = data.classes_named\n",
    "\n",
    "bar_width = 0.1\n",
    "plt.bar(np.arange(num_classes)-2.5*bar_width, label_freq, label=\"Current: Probability Mass\", width=bar_width)\n",
    "#plt.bar(np.arange(num_classes)-1.5*bar_width, max_freq/torch.sum(max_freq), label=\"Current Anno.: Majority Vote\", width=bar_width)\n",
    "\n",
    "plt.bar(np.arange(num_classes)-0.5*bar_width, label_freq2, label=\"Custom Order: Soft-Label Probability Mass\", width=bar_width)\n",
    "#plt.bar(np.arange(num_classes)+0.5*bar_width, max_freq2/torch.sum(max_freq2), label=\"Custom Order Anno.: Majority Vote\", width=bar_width)\n",
    "\n",
    "plt.bar(np.arange(num_classes)+1.5*bar_width, label_freq3, label=\"Creation ordered: Soft-Label Probability Mass\", width=bar_width)\n",
    "#plt.bar(np.arange(num_classes)+2.5*bar_width, max_freq3/torch.sum(max_freq3), label=\"Ordered Anno.: Majority Vote\", width=bar_width)\n",
    "\n",
    "\n",
    "#_ = plt.yscale(\"log\")\n",
    "_ = plt.legend()\n",
    "_ = plt.xticks(np.arange(num_classes), list(map(lambda x: x[:20], classes_named)), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "_ = plt.xlabel(\"Grouped Explanation\")\n",
    "_ = plt.ylabel(\"Proportion\")\n",
    "\n",
    "# inv_weights = 1/(max_freq/torch.sum(max_freq))\n",
    "# inv_weights /= torch.sum(inv_weights)\n",
    "# print(inv_weights.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.df[\"dataset\"] = data.df[\"group\"].apply(lambda x: 0 if x < 3 else 1 if x < 10 else 2)\n",
    "for dataset, frame in data.df.groupby(\"dataset\"):\n",
    "    slides = set(frame[\"TMA_identifier\"].unique())\n",
    "\n",
    "    inter = set(data.used_slides).intersection(slides)\n",
    "\n",
    "    idcs = sorted([data.used_slides.index(slide_name) for slide_name in inter])\n",
    "    print(idcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICE = Dice(num_classes=10, average=\"micro\")\n",
    "DICE_mac = Dice(num_classes=10, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DICE_aggreement(data, drawing_order=False):\n",
    "\n",
    "    def get_labels_with_drawing_order(data, drawing_order=False):\n",
    "        labels = []\n",
    "        bgs = []\n",
    "        for i in range(len(data)):\n",
    "            _, label, background = data.__getitem__(i, drawing_order=drawing_order, prepare_torch=False)\n",
    "            labels.append(label)\n",
    "            bgs.append(background)\n",
    "\n",
    "        return labels, bgs\n",
    "\n",
    "\n",
    "    labels, bgs = get_labels_with_drawing_order(data, drawing_order)\n",
    "\n",
    "    DICE = Dice(num_classes=10, average=\"micro\")\n",
    "    DICE_mac = Dice(num_classes=10, average=\"macro\")\n",
    "\n",
    "    for img_idx, (imgs, bg) in enumerate(zip(labels, bgs)):\n",
    "        num_annos = len(imgs)\n",
    "\n",
    "        l = torch.tensor(imgs)\n",
    "\n",
    "        for i in range(num_annos):\n",
    "            for j in range(num_annos):\n",
    "                if j > i:\n",
    "                    DICE.update(l[i],l[j])\n",
    "                    DICE_mac.update(l[i], l[j])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    print(f\"{drawing_order}: \", DICE.compute(), DICE_mac.compute())\n",
    "\n",
    "get_DICE_aggreement(data, \"grade_frame_order\")\n",
    "#get_DICE_aggreement(data, \"frame_order\")\n",
    "#get_DICE_aggreement(data, \"custom_order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = data.df.groupby(\"TMA_identifier\")[\"explanations\"].unique()\n",
    "\n",
    "ranks = np.zeros((10,10))\n",
    "\n",
    "for img in bla:\n",
    "    \n",
    "    for rank, exp in enumerate(img):\n",
    "\n",
    "        exp_num = data.exp_number_mapping[exp]\n",
    "\n",
    "        ranks[exp_num, rank] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gleason_utils import create_composite_plot\n",
    "from src import augmentations\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as wid\n",
    "\n",
    "# Again you can commnet our th rest\n",
    "@wid.interact(idx=(0,len(data)))\n",
    "def plot_compare_data(idx):\n",
    "    #_, masks1, bg = data.__getitem__(idx, False, drawing_order=\"custom_order\")\n",
    "    #_, masks2, bg = data.__getitem__(idx, False, drawing_order=\"classic\")\n",
    "    #_, masks3, bg = data.__getitem__(idx, False, drawing_order=\"frame_order\")\n",
    "    _, masks4, bg = data.__getitem__(idx, False, drawing_order=\"grade_frame_order\")\n",
    "\n",
    "\n",
    "    ref_mask = masks4[0]\n",
    "    img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(data.get_raw_image(idx)))[\"image\"]\n",
    "    masks = {\"1\":np.zeros_like(ref_mask)}\n",
    "    #masks = masks|{f\"Custom Order {i}\":mask for i, mask in enumerate(masks1)}\n",
    "    #if len(masks1) == 3:\n",
    "    #    masks = masks | {\"2\": np.zeros_like(ref_mask)}\n",
    "\n",
    "    #masks = masks | {f\"Classic Order {i}\": mask for i, mask in enumerate(masks2)}\n",
    "    #if len(masks1) == 3:\n",
    "    #    masks = masks | {\"3\": np.zeros_like(ref_mask)}\n",
    "    #masks = masks | {f\"Frame Order {i}\": mask for i, mask in enumerate(masks3)}\n",
    "    masks = masks | {f\"Grade Frame Order {i}\": mask for i, mask in enumerate(masks4)}\n",
    "\n",
    "    _ = create_composite_plot(data, None, masks, None, label_level=1, only_show_existing_annotation=True)\n",
    "#plot_compare_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_mask(data, mask, label_level, background, save_path):\n",
    "    if label_level is None:\n",
    "            label_level = data.label_level\n",
    "\n",
    "    colormap = data.colormap\n",
    "    num_class_to_vis = data.num_classes\n",
    "\n",
    "    if background is not None:\n",
    "\n",
    "        mask += 1\n",
    "        mask[cv2.resize(background.astype(np.uint8), mask.shape, interpolation=cv2.INTER_NEAREST_EXACT).astype(bool)] = 0\n",
    "\n",
    "        colormap = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), data.colormap.colors]))\n",
    "        num_class_to_vis = data.num_classes + 1\n",
    "\n",
    "\n",
    "    if save_path is not None:\n",
    "        color_palette = (colormap.colors[:, :3] * 255).astype(np.uint8)\n",
    "\n",
    "        pil_img = Image.fromarray(mask.astype(np.uint8), mode='P')\n",
    "        pil_img.putpalette(color_palette)\n",
    "        pil_img.save(save_path)\n",
    "\n",
    "    else:\n",
    "        encountered_classes = set()\n",
    "\n",
    "        f, ax = plt.subplots(1,1)\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        encountered_classes |= set(np.unique(mask))\n",
    "\n",
    "        ax.imshow(mask.astype(int), alpha=0.8,  cmap=colormap, vmin=0, vmax=num_class_to_vis, interpolation_stage=\"rgba\")\n",
    "        ax.set_axis_off()\n",
    "        return f, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_base = Path(\"figures/background_graphic/\")\n",
    "save_path_img = save_path_base/\"imgs\"\n",
    "save_path_nobg = save_path_base/\"nobg\"\n",
    "save_path_wbg = save_path_base/\"wbg\"\n",
    "save_path_bg = save_path_base/\"bg\"\n",
    "\n",
    "save_path_img.mkdir(exist_ok=True, parents=True)\n",
    "save_path_nobg.mkdir(exist_ok=True, parents=True)\n",
    "save_path_wbg.mkdir(exist_ok=True, parents=True)\n",
    "save_path_bg.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for idx in range(10):\n",
    "    _, masks, bg = data.__getitem__(idx, False)\n",
    "    img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(data.get_raw_image(idx)))[\"image\"]\n",
    "    img = Image.fromarray(img, mode=\"RGB\")\n",
    "    img.save(save_path_img/f\"{idx}.png\")\n",
    "\n",
    "    bg_img = Image.fromarray(bg.astype(np.uint8)*255, mode='L').convert(\"1\")\n",
    "    bg_img.save(save_path_bg/f\"{idx}.png\")\n",
    "\n",
    "    for a, m in enumerate(masks):\n",
    "        plot_mask(data, m, 1, None, save_path_nobg/f\"{idx}_{a}.png\")\n",
    "        plot_mask(data, m, 1, bg, save_path_wbg/f\"{idx}_{a}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GleasonXAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
