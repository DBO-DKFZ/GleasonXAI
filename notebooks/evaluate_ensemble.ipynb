{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "from src.jdt_losses import JDTLoss\n",
    "from torchmetrics import Dice\n",
    "\n",
    "from pathlib import Path\n",
    "from src.gleason_data import GleasonX\n",
    "from src.augmentations import basic_transforms_val_test_scaling512, normalize_only_transform\n",
    "import torchmetrics\n",
    "from src.model_utils import SoftDICEMetric\n",
    "from src.jdt_losses import SoftCorrectDICEMetric\n",
    "from src.jdt_losses import SoftDICECorrectAccuSemiMetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/home/experiments/Gleason\")\n",
    "data_path = Path(\"/home/datasets/GleasonXAI/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = [\"ll1/CE\", \"ll1/Tree\", \"ll1/SDB\", \"ll0/CE\", \"ll0/SDB\", \"ens/ll1/Tree\", \"ens/ll1/SDB\", \"final/ll0/CE\"]\n",
    "\n",
    "selected_model = \"final/ll1/CE\"\n",
    "remap_ll0 = False\n",
    "\n",
    "\n",
    "label_level = 1 if \"ll1\" in selected_model else 0\n",
    "eval_on = \"test\"\n",
    "save_mem = True\n",
    "\n",
    "num_classes = 10 if (label_level == 1 and not remap_ll0) else 4\n",
    "\n",
    "if remap_ll0:\n",
    "    assert \"ll1\" in selected_model\n",
    "    label_level = 0\n",
    "\n",
    "ens_mode = \"ens\" in selected_model\n",
    "\n",
    "restructured_data = \"final\" if \"final\" in selected_model else \"org\"\n",
    "\n",
    "\n",
    "if selected_model == \"ens/ll1/SDB\":\n",
    "    model_paths = [Path(f\"GleasonBackgroundMasking/label_level1/HoleMask/SoftDICEBalancedNoZoomCont-{i}/version_0/\") for i in [1,2,3,4]]\n",
    "    model_paths +=  [Path(f\"GleasonBackgroundMasking/label_level1/HoleMask/SoftDICEBalancedNoZoom-{i}/version_0/\") for i in [1, 2,3]]\n",
    "\n",
    "elif selected_model == \"ens/ll1/Tree\":\n",
    "    model_paths = [Path(f\"GleasonBackgroundMasking/label_level1/HoleMask/FinalTreeLossNoZoomCont-{i}/version_0/\") for i in [1, 2, 3, 4]]\n",
    "    model_paths +=  [Path(f\"GleasonBackgroundMasking/label_level1/HoleMask/NoZoomFinalTreeLoss-{i}/version_0/\") for i in [1, 2,3]]\n",
    "\n",
    "elif selected_model == \"ll1/CE\":\n",
    "    model_paths = [Path(f\"GleasonBackgroundMasking/label_level1/HoleMask/NoZoomFinalCE-{i}/version_0/\") for i in [1, 2, 3]]\n",
    "\n",
    "elif selected_model == \"ll1/Tree\":\n",
    "    model_paths = [Path(f\"GleasonBackgroundMasking/label_level1/HoleMask/NoZoomFinalTreeLoss-{i}/version_0/\") for i in [1, 2, 3]]\n",
    "\n",
    "elif selected_model == \"ll1/SDB\":\n",
    "    model_paths = [Path(f\"GleasonBackgroundMasking/label_level1/HoleMask/SoftDICEBalancedNoZoom-{i}/version_0/\") for i in [1, 2, 3]]\n",
    "\n",
    "elif selected_model == \"ll0/CE\":\n",
    "    model_paths = [Path(f\"GleasonBackgroundMasking/label_level0/HoleMask/NoZoomFinalCE-{i}/version_0/\") for i in [1, 2, 3]]\n",
    "\n",
    "elif selected_model == \"ll0/SDB\":\n",
    "    model_paths = [Path(f\"GleasonBackgroundMasking/label_level0/HoleMask/SoftDICEBalancedNoZoom-{i}/version_0/\") for i in [1, 2, 3]]\n",
    "\n",
    "elif selected_model == \"final/ll0/CE\":\n",
    "    model_paths = [Path(f\"GleasonFinal/label_level0/CE-{i}/version_0/\") for i in [1,2,3]]\n",
    "\n",
    "elif selected_model == \"final/ll1/CE\":\n",
    "    model_paths = [Path(f\"GleasonFinal/label_level1/CE-{i}/version_0/\") for i in [1,2,3]]\n",
    "\n",
    "else:\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "for path in model_paths:\n",
    "    assert (base_path/path).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not ens_mode or (ens_mode and not save_mem):\n",
    "\n",
    "    preds = [torch.load(base_path/m_path/\"preds\"/f\"pred_{eval_on}.pt\") for m_path in model_paths]\n",
    "    preds = [[torch.nn.functional.softmax(img_pred.float().squeeze(0), dim=0) for img_pred in pred] for pred in preds]\n",
    "    \n",
    "\n",
    "else:\n",
    "    preds_ensemble = torch.nn.functional.softmax(torch.load(base_path/model_paths[0]/\"preds\"/f\"pred_{eval_on}.pt\").float().detach(), dim=1)\n",
    "    preds = None\n",
    "    for m_path in model_paths[1:4]:\n",
    "        preds_ensemble += torch.nn.functional.softmax(torch.load(base_path/m_path/\"preds\"/f\"pred_{eval_on}.pt\").float().detach(), dim=1)\n",
    "\n",
    "    preds_ensemble = preds_ensemble/len(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_options = {\"org\":{\"scaling\":\"1024\", \"transforms\":basic_transforms_val_test_scaling512, \"label_level\":label_level, \"create_seg_masks\":True, \"tissue_mask_kwargs\":{\"open\": False, \"close\":False, \"flood\":False}},\n",
    "                \"final\": {\"scaling\": \"MicronsCalibrated\", \"transforms\": normalize_only_transform, \"label_level\": label_level, \"create_seg_masks\": True, \"tissue_mask_kwargs\": {\"open\": False, \"close\": False, \"flood\": False}, \"drawing_order\":\"grade_frame_order\", \"explanation_file\":\"final_filtered_explanations_df.csv\"}}\n",
    "\n",
    "#data = GleasonX(Path(\"/home/datasets/GleasonXAI/\"), split=\"test\", scaling=\"1024\", transforms=basic_transforms_val_test_scaling512, label_level=label_level, create_seg_masks=True, tissue_mask_kwargs={\"open\": False, \"close\":False, \"flood\":False})\n",
    "data = GleasonX(data_path, split=\"all\", **data_options[\"final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "bgs = []\n",
    "for i in range(len(data)):\n",
    "    _, label, background = data[i]\n",
    "    labels.append(label)\n",
    "    bgs.append(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tree_loss import generate_label_hierarchy\n",
    "\n",
    "def remapping_function(out):\n",
    "\n",
    "    out_remappings = generate_label_hierarchy(out, data.exp_numbered_lvl_remapping, start_level=1)\n",
    "\n",
    "    return out_remappings[0]\n",
    "\n",
    "if remap_ll0:\n",
    "    print(\"REMAP!\")\n",
    "    preds_ensemble = remapping_function(preds_ensemble)\n",
    "    #labels = remapping_function(labels)\n",
    "\n",
    "    if preds is not None:\n",
    "        preds = torch.stack([remapping_function(preds[i]) for i in range(preds.shape[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "import math\n",
    "from textwrap import wrap\n",
    "from typing import Literal\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import src.augmentations as augmentations\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from src.gleason_utils import create_composite_plot\n",
    "from matplotlib import colormaps as cm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_plot(dataset, idx, ensemble_predictions, individual_predictions, show_ensemble_preds = 0):\n",
    "    \n",
    "    img, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "    org_img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(dataset.get_raw_image(idx)))[\"image\"]\n",
    "    np_seg = np.array(ensemble_predictions[idx].argmax(dim=0)).astype(np.uint8)\n",
    "\n",
    "    masks = {\"segmentation\": np_seg} | {f\"Annotator {i}\": mask.astype(np.uint8) for i,\n",
    "                                        mask in enumerate(masks)}\n",
    "\n",
    "    sub_ensemble_preds = []\n",
    "    rnd_ensemble_subset = np.random.permutation(show_ensemble_preds)\n",
    "\n",
    "    for i in rnd_ensemble_subset:\n",
    "        np_seg = np.array(individual_predictions[i, idx].argmax(dim=0)).astype(np.uint8)\n",
    "        sub_ensemble_preds.append(np_seg)\n",
    "    \n",
    "\n",
    "    \n",
    "    if len(sub_ensemble_preds) > 0:\n",
    "        masks = {f\"ensemble_pred_{i}\":sub_ensemble_pred for i, sub_ensemble_pred in enumerate(sub_ensemble_preds)} | masks\n",
    "\n",
    "\n",
    "    f = create_composite_plot(dataset, org_img, masks, background_mask.astype(np.uint8), label_level=1, only_show_existing_annotation=True)\n",
    "\n",
    "\n",
    "def composite_prediction_plot(predictions, dataset, indices, mask_background=False, full_legend=True):\n",
    "\n",
    "    num_plots = len(indices)\n",
    "    rows = int(np.ceil(num_plots / 3))  # Assuming 3 columns per row, adjust as needed\n",
    "\n",
    "    for idx in indices:\n",
    "\n",
    "        img, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "        org_img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(dataset.get_raw_image(idx)))[\"image\"]\n",
    "\n",
    "        background = background_mask if mask_background else None\n",
    "\n",
    "        #out = generate_model_output(model, img, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)\n",
    "        out = predictions[idx]\n",
    "        out = torch.nn.functional.softmax(out, 0)\n",
    "        np_seg = np.array(out.argmax(dim=0)).astype(np.uint8)\n",
    "        \n",
    "        f = create_composite_plot(dataset, org_img,  {\"segmentation\": np_seg} | {f\"Annotator {i}\": mask for i,\n",
    "                                    mask in enumerate(masks)}, background, only_show_existing_annotation=not full_legend)\n",
    "\n",
    "        return f\n",
    "\n",
    "\n",
    "def create_single_class_acti_maps(predictions, dataset, idx, plot_mode: Literal[\"heatmap\", \"contourf\", \"contour\", \"thresholded\"] = \"contourf\", thresholds: list[float] = None, strip_background=False, plot=True):\n",
    "\n",
    "    img, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "\n",
    "    out = predictions[idx] #generate_model_output(model, img, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)\n",
    "\n",
    "    out = torch.nn.functional.softmax(out, 0)\n",
    "\n",
    "    np_seg = np.array(out.argmax(dim=0)).astype(np.uint8)\n",
    "\n",
    "    org_img = np.array(dataset.get_raw_image(idx).resize(np_seg.shape))\n",
    "\n",
    "    colormap = ListedColormap(dataset.colormap.colors)\n",
    "    num_class_to_vis = dataset.num_classes\n",
    "\n",
    "    if strip_background:\n",
    "\n",
    "        for mask in masks:\n",
    "            mask += 1\n",
    "            mask[background_mask] = 0\n",
    "\n",
    "        np_seg += 1\n",
    "        np_seg[background_mask] = 0\n",
    "\n",
    "        out[:, torch.tensor(background_mask).bool()] = 0.0\n",
    "\n",
    "        colormap = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), dataset.colormap.colors]))\n",
    "        num_class_to_vis = dataset.num_classes + 1\n",
    "\n",
    "    f, axes = plt.subplots(2, 3+math.ceil(dataset.num_classes/2), sharex=False, sharey=False, constrained_layout=False, figsize=(12, 4))\n",
    "    f.tight_layout()\n",
    "\n",
    "    axes[0, 0].imshow(org_img)\n",
    "    axes[0, 0].set_title(\"Image\", size=7)\n",
    "    axes[0, 0].set_axis_off()\n",
    "\n",
    "    # axes[1, 0].imshow(img)\n",
    "    axes[1, 0].imshow(np_seg.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1, interpolation_stage=\"rgba\")\n",
    "    axes[1, 0].set_title(\"Segmentation\", size=7)\n",
    "    axes[1, 0].set_axis_off()\n",
    "\n",
    "    for sub_ax, mask in zip_longest(list(axes[:, 1:3].flatten()), masks):\n",
    "        sub_ax.set_axis_off()\n",
    "\n",
    "        if mask is not None:\n",
    "            sub_ax.imshow(mask.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1,  interpolation_stage=\"rgba\")\n",
    "            sub_ax.set_title(\"Annotation\", size=7)\n",
    "\n",
    "    for i in range(dataset.num_classes):\n",
    "        active_axis = axes[:, 3:].flatten()[i]\n",
    "\n",
    "        class_out = out[i, :].detach().numpy()\n",
    "\n",
    "        if strip_background:\n",
    "            class_out[background_mask] = 0\n",
    "\n",
    "        temp_colormap = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), dataset.colormap.colors[i].reshape(1, -1)]))\n",
    "\n",
    "        match plot_mode:\n",
    "            case \"heatmap\": active_axis.matshow(class_out, cmap=cm[\"Grays\"].reversed(), vmin=out[out != 0.0].min(), vmax=out.max())\n",
    "            case \"multilabel\": active_axis.matshow(class_out >= 0.32, cmap=temp_colormap)\n",
    "            case \"contour\": active_axis.contour(np.flipud(class_out), cmap=cm[\"Grays\"].reversed(), vmin=0.0, vmax=out.max())\n",
    "            case \"contourf\": active_axis.contourf(np.flipud(class_out), cmap=cm[\"Grays\"].reversed(), vmin=0.0, vmax=out.max())\n",
    "            case \"thresholded\": active_axis.matshow(class_out > thresholds[i], cmap=cm[\"Grays\"].reversed())\n",
    "\n",
    "        if i == 0:\n",
    "            title = \"Benign\"\n",
    "        else:\n",
    "            exp = dataset.explanations[i-1]\n",
    "            classes_named = [\"benign tissue\", \"individual glands\", \"compressed glands\", \"poorly formed glands\",\n",
    "                             \"cribriform glands\", \"glomeruloid glands\", \"group of tumor cells\", \"single cells\", \"cords\", \"comedenocrosis\"]\n",
    "            exp_short = classes_named[i]\n",
    "            title = \"\\n\".join(wrap(str(dataset.exp_grade_mapping[exp]) + \": \" + exp_short, width=20))\n",
    "\n",
    "        active_axis.set_title(title, size=7)\n",
    "        active_axis.set_axis_off()\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
    "\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return f, axes\n",
    "\n",
    "\n",
    "def create_simple_seg_anno_plot(predictions, dataset, idx, plot_mode: Literal[\"heatmap\", \"contourf\", \"contour\", \"thresholded\"] = \"contourf\", thresholds: list[float] = None, strip_background=False, plot=True):\n",
    "\n",
    "    img, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "\n",
    "    out = predictions[idx] #generate_model_output(model, img, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)\n",
    "\n",
    "    out = torch.nn.functional.softmax(out, 0)\n",
    "\n",
    "    np_seg = np.array(out.argmax(dim=0)).astype(np.uint8)\n",
    "\n",
    "    org_img = np.array(dataset.get_raw_image(idx).resize(np_seg.shape))\n",
    "\n",
    "    colormap = ListedColormap(dataset.colormap.colors)\n",
    "    num_class_to_vis = dataset.num_classes\n",
    "\n",
    "    if strip_background:\n",
    "\n",
    "        for mask in masks:\n",
    "            mask += 1\n",
    "            mask[background_mask] = 0\n",
    "\n",
    "        np_seg += 1\n",
    "        np_seg[background_mask] = 0\n",
    "\n",
    "        out[:, torch.tensor(background_mask).bool()] = 0.0\n",
    "\n",
    "        colormap = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), dataset.colormap.colors]))\n",
    "        num_class_to_vis = dataset.num_classes + 1\n",
    "\n",
    "    f, axes = plt.subplots(1, 5, sharex=False, sharey=False, constrained_layout=False, figsize=(12, 4))\n",
    "    f.tight_layout()\n",
    "    axes[0].imshow(org_img)\n",
    "    axes[0].set_title(\"Image\", size=7)\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "    axes[1].imshow(np_seg.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1, interpolation_stage=\"rgba\")\n",
    "    axes[1].set_title(\"Segmentation\", size=7)\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "    for sub_ax, mask in zip_longest(axes[2:], masks):\n",
    "        \n",
    "        if sub_ax is None:\n",
    "            continue\n",
    "        sub_ax.set_axis_off()\n",
    "\n",
    "        if mask is not None:\n",
    "            sub_ax.imshow(mask.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1,  interpolation_stage=\"rgba\")\n",
    "            sub_ax.set_title(\"Annotation\", size=7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def create_multi_seg_anno_plot(predictions, dataset, idcs, strip_background=False):\n",
    "\n",
    "    num_plots = len(idcs)\n",
    "    \n",
    "    f, top_axes = plt.subplots(num_plots, 5, sharex=False, sharey=False, constrained_layout=False, figsize=(10, 2*num_plots))\n",
    "    f.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "    if strip_background:\n",
    "        colormap = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), dataset.colormap.colors]))\n",
    "        num_class_to_vis = dataset.num_classes + 1\n",
    "    else:\n",
    "        colormap = ListedColormap(dataset.colormap.colors)\n",
    "        num_class_to_vis = dataset.num_classes\n",
    "    \n",
    "    for i,idx in enumerate(idcs):\n",
    "\n",
    "        axes = top_axes[i]\n",
    "        \n",
    "        _, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "\n",
    "        out = predictions[idx]  # generate_model_output(model, img, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)\n",
    "\n",
    "        out = torch.nn.functional.softmax(out, 0)\n",
    "\n",
    "        np_seg = np.array(out.argmax(dim=0)).astype(np.uint8)\n",
    "\n",
    "        org_img = np.array(dataset.get_raw_image(idx).resize(np_seg.shape))\n",
    "\n",
    "\n",
    "        if strip_background:\n",
    "\n",
    "            for mask in masks:\n",
    "                mask += 1\n",
    "                mask[background_mask] = 0\n",
    "\n",
    "            np_seg += 1\n",
    "            np_seg[background_mask] = 0\n",
    "\n",
    "            out[:, torch.tensor(background_mask).bool()] = 0.0\n",
    "\n",
    "        # f, axes = plt.subplots(2, 3+math.ceil(dataset.num_classes/2), sharex=False, sharey=False, constrained_layout=False, figsize=(12, 4))\n",
    "        axes[0].imshow(org_img)\n",
    "        #axes[0].set_title(f\"Image {i}\", size=7)\n",
    "        axes[0].set_axis_off()\n",
    "\n",
    "        # axes[1, 0].imshow(img)\n",
    "        axes[1].imshow(np_seg.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1, interpolation_stage=\"rgba\")\n",
    "        #axes[1].set_title(\"Segmentation\", size=7)\n",
    "        axes[1].set_axis_off()\n",
    "\n",
    "        for sub_ax, mask in zip_longest(axes[2:], masks):\n",
    "\n",
    "            if sub_ax is None:\n",
    "                continue\n",
    "            sub_ax.set_axis_off()\n",
    "\n",
    "            if mask is not None:\n",
    "                sub_ax.imshow(mask.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1,  interpolation_stage=\"rgba\")\n",
    "                #sub_ax.set_title(\"Annotation\", size=7)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.03, hspace=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(preds, labels, bgs):\n",
    "\n",
    "    d_mac = Dice(num_classes=num_classes, average=\"macro\")\n",
    "    d_mic = Dice(num_classes=num_classes, average=\"micro\")\n",
    "    emd = []\n",
    "    mDICED = SoftDICECorrectAccuSemiMetric()\n",
    "\n",
    "    for i in range(preds.shape[0]):\n",
    "        pred = preds[i]\n",
    "        bg = bgs[i]\n",
    "        label = labels[i]\n",
    "        fg = ~bg\n",
    "\n",
    "        emd.append(((pred[:, fg] - label[:, fg]).abs().sum(dim=1)/2)/fg.sum())\n",
    "\n",
    "        label_max = torch.max(label, dim=0)[0]\n",
    "        duplicated_max = torch.sum(label == label_max.unsqueeze(0), dim=0) > 1\n",
    "\n",
    "        unique_max = ~duplicated_max\n",
    "\n",
    "        unique_max_fg = torch.logical_and(fg, unique_max)\n",
    "\n",
    "        d_mac.update(pred[:, unique_max_fg].unsqueeze(0).argmax(dim=1), label[:, unique_max_fg].unsqueeze(0).argmax(dim=1))\n",
    "        d_mic.update(pred[:, unique_max_fg].unsqueeze(0).argmax(dim=1), label[:, unique_max_fg].unsqueeze(0).argmax(dim=1))\n",
    "\n",
    "        mDICED.update(pred[:,fg].unsqueeze(0), label[:,fg].unsqueeze(0))\n",
    "\n",
    "\n",
    "    emd = torch.stack(emd).mean(dim=0)\n",
    "    emd = emd.sum()\n",
    "\n",
    "    d_mac = d_mac.compute()\n",
    "    d_mic = d_mic.compute()\n",
    "    mDICED = mDICED.compute()\n",
    "\n",
    "    return {\"mDICED\":mDICED.item(), \"L1\":emd.item(), \"DICEmacro\":d_mac.item(), \"DICE\":d_mic.item()}\n",
    "\n",
    "print(\"Model:\", selected_model, \"Ens; \", ens_mode, \"Label Level: \", label_level, \"Remaped:\", remap_ll0, \"num_classes: \", num_classes)\n",
    "\n",
    "\n",
    "eval_ens = True\n",
    "if eval_ens:\n",
    "    mets = [get_all_metrics(preds_ensemble, labels, bgs)]\n",
    "    print(mets)\n",
    "else:\n",
    "    mets = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        mets.append(get_all_metrics(preds[i], labels, bgs))\n",
    "    print(mets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mets).aggregate([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.df[\"dataset\"] = data.df[\"group\"].apply(lambda x: 0 if x < 3 else 1 if x < 10 else 2)\n",
    "for dataset, frame in data.df.groupby(\"dataset\"):\n",
    "    slides = set(frame[\"TMA_identifier\"].unique())\n",
    "\n",
    "    inter = set(data.used_slides).intersection(slides)\n",
    "\n",
    "    idcs = sorted([data.used_slides.index(slide_name) for slide_name in inter])\n",
    "\n",
    "    mets = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        preds_split = preds[i]\n",
    "\n",
    "        preds_split = preds_split[idcs]\n",
    "        labels_split = labels[idcs]\n",
    "        bgs_split = bgs[idcs]\n",
    "        mets.append(get_all_metrics(preds_split, labels_split, bgs_split))\n",
    "    print(pd.DataFrame(mets).aggregate([\"mean\", \"std\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mac = Dice(num_classes=num_classes, average=\"macro\")\n",
    "d_mic = Dice(num_classes=num_classes, average=\"micro\")\n",
    "l = JDTLoss(alpha=0.5, beta=0.5)\n",
    "\n",
    "# Compute SoftDice C,D,I\n",
    "l_C = JDTLoss(mIoUC=1.0, mIoUD=0.0, mIoUI=0.0, alpha=0.5, beta=0.5, active_classes_mode_soft=\"ALL\")\n",
    "l_D = JDTLoss(mIoUC=0.0, mIoUD=1.0, mIoUI=0.0, alpha=0.5, beta=0.5, active_classes_mode_soft=\"ALL\")\n",
    "l_I = JDTLoss(mIoUC=0.0, mIoUD=0.9, mIoUI=1.0, alpha=0.5, beta=0.5, active_classes_mode_soft=\"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd = []\n",
    "for i in range(preds_ensemble.shape[0]):\n",
    "    rel_pred = preds_ensemble[i]\n",
    "    rel_bg = bgs[i]\n",
    "    rel_label = labels[i]\n",
    "\n",
    "    emd.append(((rel_pred[:, ~rel_bg] - rel_label[:, ~rel_bg]).abs().sum(dim=1)/2)/(~rel_bg).sum())\n",
    "\n",
    "emd = torch.stack(emd).mean(dim=0)\n",
    "print(emd)\n",
    "emd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = torchmetrics.KLDivergence()\n",
    "kls = []\n",
    "\n",
    "for i in range(preds_ensemble.shape[0]):\n",
    "    rel_pred = preds_ensemble[i]\n",
    "    rel_bg = bgs[i]\n",
    "    rel_label = labels[i]\n",
    "\n",
    "    rel_pred = rel_pred[:, ~rel_bg]\n",
    "    rel_label = rel_label[:, ~rel_bg]\n",
    "\n",
    "    kls.append(torch.nn.functional.kl_div(torch.log(rel_pred).unsqueeze(0), rel_label.unsqueeze(0)))\n",
    "\n",
    "kls\n",
    "torch.mean(torch.stack(kls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tverskys = []\n",
    "active_classes = []\n",
    "per_image_dice_scores = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    bg = bgs[i]\n",
    "    label = labels[i]\n",
    "\n",
    "    tversky, ac = l.get_image_class_matrix(preds_ensemble[i][:, ~bg].unsqueeze(0), labels[i][:, ~bg].unsqueeze(0), prob_predictions=True)\n",
    "    tverskys.append(tversky)\n",
    "    active_classes.append(ac)\n",
    "\n",
    "    # Find parts of label with non-unique maximum\n",
    "    label_max = torch.max(label, dim=0)[0]\n",
    "    duplicated_max = torch.sum(label == label_max.unsqueeze(0), dim=0) > 1\n",
    "\n",
    "    unique_max = ~duplicated_max\n",
    "    fg = ~bg\n",
    "\n",
    "    unique_max_fg = torch.logical_and(fg, unique_max)\n",
    "    per_image_dice_scores.append(torchmetrics.functional.dice(preds_ensemble[i][:, unique_max_fg].unsqueeze(0).argmax(dim=1), labels[i][:, unique_max_fg].unsqueeze(0).argmax(dim=1), average=\"none\", num_classes=num_classes))\n",
    "    d_mac.update(preds_ensemble[i][:, unique_max_fg].unsqueeze(0).argmax(dim=1), labels[i][:, unique_max_fg].unsqueeze(0).argmax(dim=1))\n",
    "    d_mic.update(preds_ensemble[i][:, unique_max_fg].unsqueeze(0).argmax(dim=1), labels[i][:, unique_max_fg].unsqueeze(0).argmax(dim=1))\n",
    "tverskys = torch.stack(tverskys).squeeze()\n",
    "active_classes = torch.stack(active_classes).squeeze()\n",
    "per_image_class_dice_scores = torch.stack(per_image_dice_scores).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(per_image_class_dice_scores.nanmean(dim=0).mean(),d_mac.compute(), d_mic.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoUCs = []\n",
    "for c in range(num_classes):\n",
    "    if active_classes[:, c].sum() > 0:\n",
    "        IoUCs.append(tverskys[:, c][active_classes[:, c]].mean())\n",
    "\n",
    "mIoUC = torch.sum(torch.stack(IoUCs)) / (active_classes.sum(dim=0) > 0).sum()\n",
    "mIoUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoUIs = []\n",
    "for i in range(tverskys.shape[0]):\n",
    "    IoUIs.append(tverskys[i, :][active_classes[i, :]].mean())\n",
    "\n",
    "mIoUI = torch.sum(torch.stack(IoUIs)) / tverskys.shape[0]\n",
    "mIoUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mac_dice = SoftDICEMetric(average=\"macro\")\n",
    "other_soft_dice = SoftCorrectDICEMetric(average=None)\n",
    "\n",
    "for i in range(preds_ensemble.shape[0]):\n",
    "    rel_pred = preds_ensemble[i]\n",
    "    rel_bg = bgs[i]\n",
    "    rel_label = labels[i]\n",
    "\n",
    "    rel_pred = rel_pred[:, ~rel_bg]\n",
    "    rel_label = rel_label[:, ~rel_bg]\n",
    "\n",
    "    my_mac_dice.update(rel_pred, rel_label)\n",
    "    other_soft_dice.update(rel_pred.unsqueeze(0), rel_label.unsqueeze(0))\n",
    "\n",
    "print(my_mac_dice.compute())\n",
    "print(other_soft_dice.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jdt_losses import SoftDICECorrectAccuSemiMetric\n",
    "striped_mIoUD = SoftCorrectDICEMetric(average=\"mIoUD\")\n",
    "masked_mIoUD = []\n",
    "corrected_striped_mIoUD = SoftDICECorrectAccuSemiMetric()\n",
    "corrected_masked_mIoUD = SoftDICECorrectAccuSemiMetric()\n",
    "\n",
    "for i in range(preds_ensemble.shape[0]):\n",
    "    rel_pred = preds_ensemble[i]\n",
    "    rel_bg = bgs[i]\n",
    "    rel_label = labels[i]\n",
    "\n",
    "    rel_pred = rel_pred[:, ~rel_bg]\n",
    "    rel_label = rel_label[:, ~rel_bg]\n",
    "\n",
    "    striped_mIoUD.update(rel_pred.unsqueeze(0), rel_label.unsqueeze(0))\n",
    "    masked_mIoUD.append(1 - JDTLoss(mIoUD=1.0, mIoUC=0.0, mIoUI=0.0, active_classes_mode_soft=\"ALL\",\n",
    "                        alpha=0.5, beta=0.5)(preds_ensemble[i].unsqueeze(0), labels[i].unsqueeze(0), keep_mask=~rel_bg, prob_predictions=True))\n",
    "\n",
    "    corrected_striped_mIoUD.update(rel_pred.unsqueeze(0), rel_label.unsqueeze(0))\n",
    "    corrected_masked_mIoUD.update(preds_ensemble[i].unsqueeze(0), labels[i].unsqueeze(0), keep_mask=~rel_bg)\n",
    "masked_mIoUD = torch.tensor(masked_mIoUD).mean()\n",
    "\n",
    "\n",
    "print(striped_mIoUD.compute())\n",
    "print(masked_mIoUD)\n",
    "print(corrected_striped_mIoUD.compute())\n",
    "print(corrected_masked_mIoUD.compute())\n",
    "print(1 - JDTLoss(mIoUD=1.0, mIoUC=0.0, mIoUI=0.0, active_classes_mode_soft=\"ALL\", alpha=0.5, beta=0.5)(preds_ensemble, labels, keep_mask=~bgs, prob_predictions=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_l_label = []\n",
    "super_l_pred = []\n",
    "for i in range(preds_ensemble.shape[0]):\n",
    "    rel_pred = preds_ensemble[i]\n",
    "    rel_bg = bgs[i]\n",
    "    rel_label = labels[i]\n",
    "\n",
    "    rel_pred = rel_pred[:, ~rel_bg]\n",
    "    rel_label = rel_label[:, ~rel_bg]\n",
    "\n",
    "    super_l_label.append(rel_label)\n",
    "    super_l_pred.append(rel_pred)\n",
    "\n",
    "super_l_pred = torch.cat(super_l_pred, dim=1)\n",
    "super_l_label = torch.cat(super_l_label, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_soft_dice = SoftCorrectDICEMetric(average=None)\n",
    "other_soft_dice(super_l_pred.unsqueeze(0), super_l_label.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - JDTLoss(mIoUD=1.0, mIoUC=0.0, mIoUI=0.0, active_classes_mode_soft=\"ALL\", alpha=0.5, beta=0.5)(super_l_pred.unsqueeze(0), super_l_label.unsqueeze(0), prob_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jdt_losses import JDTLoss\n",
    "\n",
    "1- JDTLoss(mIoUD=1.0, mIoUC=0.0, mIoUI=0.0, active_classes_mode_soft=\"ALL\", alpha=0.5, beta=0.5)(preds_ensemble, labels, keep_mask=~bgs, prob_predictions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from tqdm import tqdm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import ipywidgets as wid\n",
    "\n",
    "from src.jdt_losses import SoftCorrectDICEMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_save_path = Path(\"./figures\")\n",
    "img_save_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute outputs\n",
    "\n",
    "STRIP_BACKGROUND = True\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "pix_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "max_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "pred_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "ml_pred_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "\n",
    "one_annotator_pred_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "\n",
    "\n",
    "conf_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n",
    "conf_matrix_ml = ConfusionMatrix(task=\"multilabel\", num_labels=num_classes)\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "\n",
    "    out = preds_ensemble[i]\n",
    "\n",
    "    background_mask = bgs[i]\n",
    "    mask = labels[i]\n",
    "    if STRIP_BACKGROUND:\n",
    "        foreground_mask = ~background_mask.bool()\n",
    "    else:\n",
    "        foreground_mask = torch.ones_like(background_mask).bool()\n",
    "\n",
    "    label_max = torch.max(mask, dim=0)[0]\n",
    "    duplicated_max = torch.sum(mask == label_max.unsqueeze(0), dim=0) > 1\n",
    "\n",
    "    unique_max = ~duplicated_max\n",
    "\n",
    "    unique_max_fg = torch.logical_and(foreground_mask, unique_max)\n",
    "\n",
    "\n",
    "\n",
    "    mask_unique = mask[:, unique_max_fg]\n",
    "    out_unique = out[:, unique_max_fg]\n",
    "\n",
    "    mask = mask[:, foreground_mask].flatten(start_dim=1)\n",
    "    out = out[:, foreground_mask].flatten(start_dim=1)\n",
    "\n",
    "    pix_freq += torch.sum((mask > 0), dim=(1))\n",
    "    max_freq += torch.bincount(torch.argmax(mask_unique, dim=0).reshape(-1), minlength=num_classes)\n",
    "    pred_freq += torch.bincount(torch.argmax(out,  dim=0).reshape(-1), minlength=num_classes)\n",
    "    one_annotator_pred_freq += torch.sum(out >= 0.33, dim=(1))\n",
    "\n",
    "    conf_matrix(out_unique.argmax(dim=0), mask_unique.argmax(dim=0))\n",
    "    conf_matrix_ml(out.T >= 0.33, mask.T >= 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "classes_named = data.classes_named\n",
    "classes_named = [\"benign tissue\", \"individual glands\", \"compressed glands\", \"poorly formed glands\", \"cribriform glands\", \"glomeruloid glands\", \"group of tumor cells\", \"single cells\", \"cords\", \"comedenocrosis\"]\n",
    "confm = conf_matrix.compute()\n",
    "\n",
    "confm_normed = confm / confm.sum(dim=1).reshape(-1, 1)\n",
    "confm_normed = (confm_normed * 1000).round().long()\n",
    "\n",
    "# confm = confm / torch.sum(confm, dim=1, keepdim=True)\n",
    "\n",
    "#confm_expanded = torch.zeros(confm.shape[0]+1, confm.shape[1]+1, dtype=int)\n",
    "#confm_expanded[:confm.shape[0], :confm.shape[1]] = confm_normed\n",
    "\n",
    "#proportion_gt = (torch.sum(confm, dim=1) / torch.sum(confm)) * 1000\n",
    "#proportion_predicted = (torch.sum(confm, dim=0) / torch.sum(confm)) * 1000\n",
    "\n",
    "\n",
    "# Calculate proportions\n",
    "\n",
    "#confm_expanded[-1, :-1] = proportion_predicted.round().long()\n",
    "#confm_expanded[:-1, -1] = proportion_gt.round().long()\n",
    "\n",
    "confm_expanded = confm_normed\n",
    "original_cmap = cm.get_cmap(\"Blues\")\n",
    "im = plt.matshow(confm_expanded, cmap=LinearSegmentedColormap.from_list(\"Blues_80\", original_cmap(np.linspace(0, 0.7, 256))))\n",
    "\n",
    "for (i, j), val in np.ndenumerate(confm_expanded):\n",
    "    plt.text(j, i, f'{val}', ha='center', va='center', color='black')\n",
    "\n",
    "#plt.colorbar(im)\n",
    "plt.xticks(range(num_classes), list(map(lambda x: x, classes_named)), #+ [\"Proportion\"],\n",
    "           rotation=45, rotation_mode=\"anchor\", ha=\"right\", va=\"center_baseline\")\n",
    "\n",
    "plt.yticks(range(num_classes), list(map(lambda x: x, classes_named)) #+ [\"Proportion\"]\n",
    "           , rotation=45)\n",
    "plt.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "\n",
    "plt.ylabel(\"Annotations\")\n",
    "plt.xlabel(\"Predictions\")\n",
    "\n",
    "# plt.xlabel(\"Predictions\")\n",
    "# plt.ylabel(\"GT\")\n",
    "plt.savefig(img_save_path / \"conf_mat.png\", dpi=1000)\n",
    "\n",
    "\n",
    "# per_class_acc = ((confm.diagonal() / confm.sum(dim=1)) * 1000).round().long()\n",
    "\n",
    "# r = torch.stack([per_class_acc, proportion_predicted.round().long(), proportion_gt.round().long()])\n",
    "# im2 = plt.matshow(r)\n",
    "# plt.colorbar(im2)\n",
    "# for (i, j), val in np.ndenumerate(r):\n",
    "#     plt.text(j, i, f\"{val}\", c=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class annotation and prediction frequency\n",
    "\n",
    "a = preds_ensemble.sum(dim=(0, 2, 3))\n",
    "a /= a.sum()\n",
    "\n",
    "b = labels.sum(dim=(0, 2, 3))\n",
    "b /= b.sum()\n",
    "\n",
    "\n",
    "bar_width = 0.15\n",
    "plt.bar(np.arange(num_classes)-1.7*bar_width, b, label=\"Soft-Label Probability Mass\", width=bar_width)\n",
    "plt.bar(np.arange(num_classes)-0.7*bar_width, a, label=\"Predictive Probability Mass\", width=bar_width)\n",
    "\n",
    "plt.bar(np.arange(num_classes)+0.7*bar_width, max_freq/torch.sum(max_freq), label=\"Anno.: Majority Vote\", width=bar_width)\n",
    "# plt.bar(np.arange(num_classes)+1.5*bar_width, one_annotator_pred_freq/torch.sum(max_freq), label=\"one_annotator_pred_freq\", width=bar_width)\n",
    "plt.bar(np.arange(num_classes)+1.7*bar_width, pred_freq/torch.sum(pred_freq), label=\"Pred.: Argmax\", width=bar_width)\n",
    "\n",
    "# plt.yscale(\"log\")\n",
    "_ = plt.legend()\n",
    "_ = plt.xticks(np.arange(num_classes), list(map(lambda x: x[:20], classes_named)), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "_ = plt.xlabel(\"Grouped Explanation\")\n",
    "_ = plt.ylabel(\"Proportion\")\n",
    "\n",
    "plt.savefig(img_save_path / \"prob_mass.png\", dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm_ml = conf_matrix_ml.compute()\n",
    "confm_ml_reshape = confm_ml.reshape(confm_ml.shape[0], -1)\n",
    "num_pixels = confm_ml_reshape.sum(dim=1)\n",
    "confm_ml_reshape_normed = confm_ml_reshape / num_pixels.reshape(-1, 1)\n",
    "\n",
    "TN, FP, FN, TP = confm_ml_reshape[:, 0], confm_ml_reshape[:, 1], confm_ml_reshape[:, 2], confm_ml_reshape[:, 3]\n",
    "\n",
    "P = (TP+FN)\n",
    "N = (TN+FP)\n",
    "\n",
    "POPU = (TN+FP+FN+TP)\n",
    "ACC = (TN+TP)/POPU\n",
    "TPR = TP/(TP+FN)\n",
    "TNR = (TN/(TN+FP))\n",
    "PREC = TP/(FP+TP)\n",
    "BACC = (TPR+TNR) / 2\n",
    "DICE = (2*TP)/(2*TP+FP+FN)\n",
    "\n",
    "PREL = P / POPU\n",
    "\n",
    "confm_ml_reshape_normed = torch.cat([PREL.reshape(num_classes, 1), confm_ml_reshape_normed[:, [3,0,1,2]], ACC.reshape(\n",
    "    num_classes, 1), PREC.reshape(num_classes, 1), BACC.reshape(num_classes, 1), DICE.reshape(num_classes,1)], dim=1)\n",
    "\n",
    "im3 = plt.matshow(confm_ml_reshape_normed)\n",
    "plt.colorbar(im3)\n",
    "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7,8], [\"PREL\", \"TP\", \"TN\", \"FP\", \"FN\", \"ACC\", \"PREC\", \"BA\", \"DICE\"], rotation=45)\n",
    "_ = plt.yticks(range(len(classes_named)), classes_named)\n",
    "_ = plt.xlabel(\"Metrics in %\")\n",
    "for (i, j), val in np.ndenumerate(confm_ml_reshape_normed):\n",
    "    plt.text(j, i, f\"{val*1000:0.0f}\",c=\"red\", ha=\"center\", va=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, idcs = torch.sort(torch.stack(IoUIs), descending=True)\n",
    "\n",
    "@wid.interact(idx=wid.IntSlider(min=0, max=len(data)-1, value=0))\n",
    "def show_worst_results(idx):\n",
    "    _ = create_single_class_acti_maps(predictions=preds_ensemble, dataset=data, idx=idcs[idx], plot_mode=\"heatmap\", strip_background=True)\n",
    "    #_ = create_ensemble_plot(dataset=data, ensemble_predictions=preds_ensemble, individual_predictions=preds, idx=idcs[idx], show_ensemble_preds=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, idcs = torch.sort(torch.stack(IoUIs), descending=True)\n",
    "\n",
    "def show_worst_results(idx):\n",
    "    f, ax = create_single_class_acti_maps(predictions=preds_ensemble, dataset=data, idx=idcs[20], plot_mode=\"heatmap\", strip_background=True, plot=False)\n",
    "    plt.subplots_adjust\n",
    "    plt.savefig(img_save_path / \"test.png\", dpi=2000)\n",
    "    #_ = create_ensemble_plot(dataset=data, ensemble_predictions=preds_ensemble, individual_predictions=preds, idx=idcs[idx], show_ensemble_preds=0)\n",
    "\n",
    "show_worst_results(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, idcs = torch.sort(torch.stack(IoUIs), descending=True)\n",
    "imgs_good = np.array([3,5,6,9,10,11,14,15,16,18,20,21,23,24,34,36,45,46,92])\n",
    "sm_plot = np.array([25,46])\n",
    "\n",
    "sieht_anders = np.array([49, 56, 58, 70, 63, 91, 17])\n",
    "\n",
    "ganz_anders =np.array([62,68,73, 26])\n",
    "\n",
    "create_multi_seg_anno_plot(predictions=preds_ensemble, dataset=data, idcs=[idcs[i] for i in sm_plot], strip_background=True)\n",
    "plt.savefig(img_save_path / \"imgs_good.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_vis = np.concatenate([imgs_good[[2,5,7,8,16]], sieht_anders[[2,6]], ganz_anders[[0,3]]], axis=0)\n",
    "create_multi_seg_anno_plot(predictions=preds_ensemble, dataset=data, idcs=[idcs[i] for i in to_vis], strip_background=True)\n",
    "plt.savefig(img_save_path / \"imgs_good_selected.png\", dpi=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GleasonXAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
