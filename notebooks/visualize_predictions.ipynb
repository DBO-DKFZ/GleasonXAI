{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import src.augmentations as augmentations\n",
    "import src.model_utils\n",
    "from src.gleason_data import GleasonX, prepare_torch_inputs\n",
    "from src.gleason_utils import create_composite_plot\n",
    "from src.lightning_modul import LitSegmenter\n",
    "import src.tree_loss as tree_loss\n",
    "from matplotlib import colormaps as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from src.tree_loss import generate_label_hierarchy\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "from src.lightning_modul import LitClassifier\n",
    "from src.gleason_data import GleasonXClassification\n",
    "from itertools import zip_longest\n",
    "import math\n",
    "from textwrap import wrap\n",
    "import torchvision.transforms as tt\n",
    "from ipywidgets import widgets\n",
    "from monai.inferers import *\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from torch.utils.data import Subset\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets.widgets as wid\n",
    "from PIL import Image\n",
    "import albumentations as alb\n",
    "import ipywidgets\n",
    "\n",
    "from src.gleason_data import get_class_colormaps\n",
    "\n",
    "try:\n",
    "    sys.modules[\"tree_loss\"] = sys.modules[\"src.tree_loss\"]\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sys.modules[\"robust_loss_functions\"] = sys.modules[\"src.robust_loss_functions\"]\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sys.modules[\"loss_functions\"] = sys.modules[\"src.loss_functions\"]\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sys.modules[\"augmentations\"] = sys.modules[\"src.augmentations\"]\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    sys.modules[\"model_utils\"] = sys.modules[\"src.model_utils\"]\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/datasets/GleasonXAI\"\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "\n",
    "EXPERIMENT_PATH = \"home/experiments/Gleason\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT function\n",
    "\n",
    "LABEL_REMAPPING = None\n",
    "SLIDING_WINDOW_INFERER = None\n",
    "\n",
    "\n",
    "def generate_model_output(model, img, device=\"cpu\", label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER):\n",
    "    model.eval()\n",
    "\n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = tt.functional.to_tensor(img)\n",
    "\n",
    "    if len(img.size()) == 3:\n",
    "        no_batch_input = True\n",
    "        img = img.unsqueeze(0)\n",
    "    else:\n",
    "        no_batch_input = False\n",
    "\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if inferer is not None:\n",
    "            out = inferer(img, model)\n",
    "        else:\n",
    "            out = model(img)\n",
    "\n",
    "    # Move back and strip batch_dim\n",
    "    out = out.cpu()\n",
    "\n",
    "    if label_remapping is not None:\n",
    "        out = label_remapping(out)\n",
    "\n",
    "    if no_batch_input:\n",
    "        out = out[0, ...]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_prediction_plot(model, dataset, indices, mask_background=False, full_legend=True):\n",
    "\n",
    "    num_plots = len(indices)\n",
    "    rows = int(np.ceil(num_plots / 3))  # Assuming 3 columns per row, adjust as needed\n",
    "\n",
    "    # f, axs = plt.subplots(rows, 3, figsize=(12, 4 * rows))\n",
    "\n",
    "    # for idx, ax in zip(indices, axs.flatten()):\n",
    "    for idx in indices:\n",
    "\n",
    "        img, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "        org_img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(dataset.get_raw_image(idx)))[\"image\"]\n",
    "\n",
    "        background = background_mask if mask_background else None\n",
    "\n",
    "        if model is not None:\n",
    "            out = generate_model_output(model, img, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)\n",
    "            out = torch.nn.functional.softmax(out, 0)\n",
    "            np_seg = np.array(out.argmax(dim=0)).astype(np.uint8)\n",
    "            #out = generate_model_output(model, img, masks, dataset.num_classes)\n",
    "            #np_seg = np.array(out.squeeze(0).argmax(dim=0)).astype(np.uint8)\n",
    "            f = create_composite_plot(dataset, org_img,  {\"segmentation\": np_seg} | {f\"Annotator {i}\": mask for i, mask in enumerate(masks)}, background, only_show_existing_annotation=not full_legend)\n",
    "        else:\n",
    "            f = create_composite_plot(dataset, org_img, {f\"Annotator {i}\": mask for i, mask in enumerate(masks)}, background, only_show_existing_annotation=not full_legend)\n",
    "\n",
    "        return f\n",
    "    # f.show()\n",
    "\n",
    "\n",
    "def create_single_class_acti_maps(model, dataset, idx, plot_mode: Literal[\"heatmap\", \"contourf\", \"contour\", \"thresholded\"] = \"contourf\", thresholds: list[float] = None, strip_background=False):\n",
    "\n",
    "    img, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "    # org_img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(dataset.get_raw_image(idx)))[\"image\"]\n",
    "\n",
    "    out = generate_model_output(model, img, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)\n",
    "\n",
    "    out = torch.nn.functional.softmax(out, 0)\n",
    "\n",
    "    np_seg = np.array(out.argmax(dim=0)).astype(np.uint8)\n",
    "\n",
    "    org_img = np.array(dataset.get_raw_image(idx).resize(np_seg.shape))\n",
    "\n",
    "    colormap = ListedColormap(dataset.colormap.colors)\n",
    "    num_class_to_vis = dataset.num_classes\n",
    "\n",
    "    if strip_background:\n",
    "\n",
    "        for mask in masks:\n",
    "            mask += 1\n",
    "            mask[background_mask] = 0\n",
    "\n",
    "        np_seg += 1\n",
    "        np_seg[background_mask] = 0\n",
    "\n",
    "        out[:, torch.tensor(background_mask).bool()] = 0.0\n",
    "        \n",
    "        colormap = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), dataset.colormap.colors]))\n",
    "        num_class_to_vis = dataset.num_classes + 1\n",
    "\n",
    "    f, axes = plt.subplots(2, 3+math.ceil(dataset.num_classes/2), sharex=False, sharey=False, constrained_layout=False, figsize=(12, 4))\n",
    "    f.tight_layout()\n",
    "\n",
    "    axes[0, 0].imshow(org_img)\n",
    "    axes[0, 0].set_title(\"Image\", size=7)\n",
    "    axes[0, 0].set_axis_off()\n",
    "\n",
    "    # axes[1, 0].imshow(img)\n",
    "    axes[1, 0].imshow(np_seg.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1, interpolation_stage=\"rgba\")\n",
    "    axes[1, 0].set_title(\"Segmentation\", size=7)\n",
    "    axes[1, 0].set_axis_off()\n",
    "\n",
    "    for sub_ax, mask in zip_longest(list(axes[:, 1:3].flatten()), masks):\n",
    "        sub_ax.set_axis_off()\n",
    "\n",
    "        if mask is not None:\n",
    "            sub_ax.imshow(mask.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1,  interpolation_stage=\"rgba\")\n",
    "            sub_ax.set_title(\"Annotation\", size=7)\n",
    "\n",
    "    for i in range(dataset.num_classes):\n",
    "        active_axis = axes[:, 3:].flatten()[i]\n",
    "\n",
    "        class_out = out[i, :].detach().numpy()\n",
    "\n",
    "        if strip_background:\n",
    "            class_out[background_mask] = 0\n",
    "\n",
    "        temp_colormap = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), dataset.colormap.colors[i].reshape(1,-1)]))\n",
    "\n",
    "        match plot_mode:\n",
    "            case \"heatmap\": active_axis.matshow(class_out, cmap=cm[\"Grays\"].reversed(), vmin=0.0, vmax=out.max())\n",
    "            case \"multilabel\": active_axis.matshow(class_out >= 0.32 , cmap=temp_colormap)\n",
    "            case \"contour\": active_axis.contour(np.flipud(class_out), cmap=cm[\"Grays\"].reversed(), vmin=0.0, vmax=out.max())\n",
    "            case \"contourf\": active_axis.contourf(np.flipud(class_out), cmap=cm[\"Grays\"].reversed(), vmin=0.0, vmax=out.max())\n",
    "            case \"thresholded\": active_axis.matshow(class_out > thresholds[i], cmap=cm[\"Grays\"].reversed())\n",
    "\n",
    "        if i == 0:\n",
    "            title = \"Benign\"\n",
    "        else:\n",
    "            exp = dataset.explanations[i-1]\n",
    "            title = \"\\n\".join(wrap(str(dataset.exp_grade_mapping[exp]) + \": \" + exp, width=20))\n",
    "\n",
    "        active_axis.set_title(title, size=7)\n",
    "        active_axis.set_axis_off()\n",
    "\n",
    "    #from matplotlib.colors import Normalize\n",
    "    #from matplotlib.cm import ScalarMappable\n",
    "    #cbar_ax = f.add_axes([0.93, 0.15, 0.02, 0.7])  #\n",
    "    #norm = Normalize(vmin=0, vmax=out.max())  # Assuming out.max() represents the maximum value in your data\n",
    "    #sm = ScalarMappable(norm=norm, cmap=cm[\"Grays\"].reversed())\n",
    "    #sm.set_array([])  # Required for the colorbar to work properly\n",
    "\n",
    "    ## Add the colorbar to the axis\n",
    "    #cbar = plt.colorbar(sm, ax=active_axis)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model, Dataset and Option Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CONSTANTS\n",
    "\n",
    "# What changed: SegmentationModelsTest V0: Normalization, RandomResizedCrops, CenterCrop statt RandomCrop in val (marginaler unterschied), smpUNet statt unet.py (pretrained), adamW statt adam, größere LR, größerer WeightDecay, precision=16\n",
    "#               SegmentationModelsTest V3: Batchsize to 32 from 8.\n",
    "\n",
    "# Use 16-mixed instead. Testing different loss functions. Testing color and distortion transforms.\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 8\n",
    "# SCALING = \"1024\"\n",
    "# LABEL_LEVEL = 1\n",
    "\n",
    "# dataset = GleasonX(DATA_PATH, \"train\", scaling=SCALING, transforms=transforms_val_test, label_level=LABEL_LEVEL)\n",
    "# dataset_val = GleasonX(DATA_PATH, \"val\", scaling=SCALING, transforms=transforms_val_test, label_level=LABEL_LEVEL)\n",
    "# dataset_test = GleasonX(DATA_PATH, \"test\", scaling=SCALING, transforms=transforms_val_test, label_level=LABEL_LEVEL)\n",
    "# dataset_all = GleasonX(DATA_PATH, \"all\", scaling=SCALING, transforms=transforms_val_test, label_level=LABEL_LEVEL)\n",
    "\n",
    "# dataset_Gleason = GleasonX(DATA_PATH, \"test\", scaling=SCALING, transforms=transforms_val_test, label_level=0)\n",
    "\n",
    "\n",
    "# NUM_CLASSES = dataset.num_classes\n",
    "# CLASSES_NAMED = [\"Background\"] + dataset_val.explanations\n",
    "\n",
    "# dataloader_train = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "# dataloader_val = DataLoader(dataset=dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "# dataloader_test = DataLoader(dataset=dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)#\n",
    "\n",
    "# dataloader_Gleason = DataLoader(dataset=dataset_Gleason, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SELECTION WIDGET\n",
    "\n",
    "# MODEL_BASE_PATHS = [Path(p) for p in  [\"/home/experiments/GleasonImprovements\",\n",
    "#                    \"/home/experiments/GleasonBackgroundMasking\", \"/home/experiments/GleasonClassification\"]]\n",
    "MODEL_BASE_PATHS = [Path(p) for p in [EXPERIMENT_PATH]]\n",
    "\n",
    "\n",
    "model = None\n",
    "device = None\n",
    "import os\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    def find_best_models(base_paths):\n",
    "        exp_dict = {}\n",
    "        for base_path in base_paths:\n",
    "            for model_path in Path(base_path).glob(\"**/*.ckpt\"):\n",
    "                experiment_path = Path(model_path).relative_to(base_path.parent)\n",
    "                *experiment_super_folders, experiment_name, version_number, checkpoints, model_name = experiment_path.parts\n",
    "\n",
    "                try:\n",
    "                    experiment_super_folder = str(Path(os.path.join(*experiment_super_folders[1:])))\n",
    "                    exp_name = f\"{experiment_super_folder}/{experiment_name}/{version_number}\"\n",
    "\n",
    "                except:\n",
    "                    exp_name = f\"{experiment_super_folder}/{experiment_name}/{version_number}\"\n",
    "\n",
    "                if exp_name in exp_dict and not \"best_model\" in model_name:\n",
    "                    continue\n",
    "                exp_dict[exp_name] = model_path\n",
    "        return exp_dict\n",
    "\n",
    "    # Define base path\n",
    "\n",
    "    # Find best model checkpoints and extract experiment names\n",
    "    best_models = find_best_models(MODEL_BASE_PATHS)\n",
    "    device_opts = {\"cpu\": torch.device(\"cpu\"), \"gpu\": torch.device(\"cuda:0\")}\n",
    "\n",
    "    MODEL_PATH = None\n",
    "\n",
    "    def handle_dropdown_change(*args):\n",
    "        nonlocal MODEL_PATH\n",
    "        global device, model\n",
    "\n",
    "        selected_exp_name = model_dropdown.value\n",
    "        MODEL_PATH = best_models[selected_exp_name]\n",
    "        selected_device = device_dropdown.value\n",
    "        device = device_opts[selected_device]\n",
    "        model = LitSegmenter.load_from_checkpoint(str(MODEL_PATH), map_location=device)\n",
    "        print(\"Model Loaded!\")\n",
    "\n",
    "    print(best_models)\n",
    "    # Create dropdown widget\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=sorted(best_models.keys()),\n",
    "        description='Select Model:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    device_dropdown = widgets.Dropdown(\n",
    "        options=device_opts.keys(),\n",
    "        description='Select Device:',\n",
    "        disabled=False,\n",
    "        value=\"gpu\",\n",
    "    )\n",
    "\n",
    "    model_menu = widgets.VBox([model_dropdown, device_dropdown])\n",
    "\n",
    "    handle_dropdown_change()\n",
    "    # Display dropdown\n",
    "    model_dropdown.observe(handle_dropdown_change)\n",
    "    device_dropdown.observe(handle_dropdown_change)\n",
    "\n",
    "    return model_menu\n",
    "\n",
    "\n",
    "_model_menu = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATSET SELECTION WIDGET\n",
    "STATISTICS_DATASET = None  # dataset_Gleason\n",
    "DATALOADER = None\n",
    "num_classes = None\n",
    "classes_named = None\n",
    "LABEL_LEVEL = None\n",
    "# transforms_train = augmentations.tellez_transforms_train\n",
    "data_transform = augmentations.basic_transforms_val_test_scaling2048\n",
    "\n",
    "STRIP_BACKGROUND = True\n",
    "\n",
    "\n",
    "def create_statistics_datset():\n",
    "    split_opts = [\"all\", \"train\", \"val\", \"test\"]\n",
    "    label_level_opts = [0, 1, 2, 3]\n",
    "    scaling_opts = [\"original\", \"1024\", \"2048\"]\n",
    "    transform_opts = [\"resize_512\", \"resize_1024\", \"resize_2048\"]\n",
    "    background_mask_opts = [\"all\", \"without_holes\"]\n",
    "    annotation_file_opts = [\"fine_only_explanations.csv\", \"explanations_df.csv\"]\n",
    "    label_remapping_files_opts = [\"label_remapping.json\", \"label_remapping_coarser.json\"]\n",
    "\n",
    "    split_dropdown = widgets.Dropdown(options=split_opts, description='Split:', value=\"val\")\n",
    "    label_level_dropdown = widgets.Dropdown(options=label_level_opts, description='Label Level:', value=0)\n",
    "    scaling_dropdown = widgets.Dropdown(options=scaling_opts, description='Scaling:', value=\"original\")\n",
    "    transform_dropdown = widgets.Dropdown(options=transform_opts, description='Transform:', value=\"resize_1024\")\n",
    "    explanation_file_dropdown = widgets.Dropdown(options=annotation_file_opts, description='Annotation file:', value=\"explanations_df.csv\")\n",
    "    label_remapping_file_dropdown = widgets.Dropdown(options=label_remapping_files_opts, description='Remapping file:', value=\"label_remapping.json\")\n",
    "\n",
    "    background_mask_toggle = widgets.ToggleButtons(options=background_mask_opts, description='Background Masking:', value=\"without_holes\")\n",
    "\n",
    "    statistics_dataset_menu = widgets.VBox([split_dropdown, label_level_dropdown, scaling_dropdown, transform_dropdown,\n",
    "                                           explanation_file_dropdown, label_remapping_file_dropdown, background_mask_toggle])\n",
    "\n",
    "    strip_background_checkbox = widgets.Checkbox(description=\"Strip Background\", value=True)\n",
    "\n",
    "    sliding_window_averaging_opts = [\"constant\", \"gaussian\"]\n",
    "\n",
    "    sliding_window_checkbox = widgets.Checkbox(description='Use Sliding Window:', value=True)\n",
    "    sliding_window_averaging_dropdown = widgets.Dropdown(options=sliding_window_averaging_opts, description='SlidingWindowAveraging:', value=\"gaussian\")\n",
    "    sliding_window_overlap_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='SlidingWindowOverlap:')\n",
    "\n",
    "    sliding_window_menu = widgets.VBox([sliding_window_checkbox, strip_background_checkbox, sliding_window_averaging_dropdown, sliding_window_overlap_slider])\n",
    "\n",
    "    # Define dataset creation function\n",
    "\n",
    "    def create_dataset(*args):\n",
    "        global STATISTICS_DATASET, DATALOADER, LABEL_LEVEL, num_classes, classes_named\n",
    "\n",
    "        CHOSEN_SPLIT = split_dropdown.value\n",
    "        CHOSEN_LABEL_LEVEL = label_level_dropdown.value\n",
    "        CHOSEN_SCALING = scaling_dropdown.value\n",
    "        LABEL_LEVEL = CHOSEN_LABEL_LEVEL\n",
    "        CHOSEN_TRANSFORM = transform_dropdown.value\n",
    "        CHOSEN_ANNOTATION_FILE = explanation_file_dropdown.value\n",
    "        CHOSEN_REMAPPING_FILE = label_remapping_file_dropdown.value\n",
    "\n",
    "        CHOSEN_BACKGROUND_OPT = background_mask_toggle.value\n",
    "\n",
    "        CHOSEN_TRANSFORM = {\"resize_1024\": augmentations.basic_transforms_val_test_scaling1024,\n",
    "                            \"resize_2048\": augmentations.basic_transforms_val_test_scaling2048,\n",
    "                            \"resize_512\": augmentations.basic_transforms_val_test_scaling512}[CHOSEN_TRANSFORM]\n",
    "\n",
    "        CHOSEN_BACKGROUND_OPT = {\"all\": {}, \"without_holes\": {\"open\": False, \"close\": False, \"flood\": False}}[CHOSEN_BACKGROUND_OPT]\n",
    "\n",
    "        STATISTICS_DATASET = GleasonX(DATA_PATH, CHOSEN_SPLIT, scaling=CHOSEN_SCALING, transforms=CHOSEN_TRANSFORM,\n",
    "                                      label_level=CHOSEN_LABEL_LEVEL, create_seg_masks=True, tissue_mask_kwargs=CHOSEN_BACKGROUND_OPT, explanation_file=CHOSEN_ANNOTATION_FILE, label_hierarchy_file=CHOSEN_REMAPPING_FILE)\n",
    "        DATALOADER = DataLoader(dataset=STATISTICS_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        num_classes = STATISTICS_DATASET.num_classes\n",
    "        classes_named = STATISTICS_DATASET.classes_named\n",
    "        print(\"Called with\", str(CHOSEN_SPLIT), str(CHOSEN_LABEL_LEVEL), str(CHOSEN_SCALING), str(CHOSEN_TRANSFORM), str(CHOSEN_BACKGROUND_OPT), str(CHOSEN_ANNOTATION_FILE), str(CHOSEN_REMAPPING_FILE))\n",
    "\n",
    "    create_dataset()\n",
    "    split_dropdown.observe(create_dataset, names='value')\n",
    "    label_level_dropdown.observe(create_dataset, names='value')\n",
    "    scaling_dropdown.observe(create_dataset, names='value')\n",
    "    transform_dropdown.observe(create_dataset, names='value')\n",
    "    explanation_file_dropdown.observe(create_dataset, names=\"value\")\n",
    "    label_remapping_file_dropdown.observe(create_dataset, names=\"value\")\n",
    "\n",
    "    background_mask_toggle.observe(create_dataset, names=\"values\")\n",
    "\n",
    "    statistics_dataset_menu.observe(create_dataset)\n",
    "\n",
    "    def create_label_remapper(*args):\n",
    "        global LABEL_REMAPPING\n",
    "\n",
    "        MODEL_LEVEL = model_label_lvl.value\n",
    "        DATA_LABEL_LEVEL = label_level_dropdown.value\n",
    "\n",
    "        if MODEL_LEVEL == DATA_LABEL_LEVEL:\n",
    "            LABEL_REMAPPING = None\n",
    "            print(f\"Created remapping from {MODEL_LEVEL} to {DATA_LABEL_LEVEL}. No mapping\")\n",
    "        else:\n",
    "            dataset_number_remappings = STATISTICS_DATASET.exp_numbered_lvl_remapping\n",
    "\n",
    "            def remapping_function(out):\n",
    "\n",
    "                out_remappings = generate_label_hierarchy(out, dataset_number_remappings, start_level=MODEL_LEVEL)\n",
    "\n",
    "                return out_remappings[DATA_LABEL_LEVEL]\n",
    "\n",
    "            LABEL_REMAPPING = remapping_function\n",
    "            print(f\"Created remapping from {MODEL_LEVEL} to {DATA_LABEL_LEVEL}.\")\n",
    "\n",
    "    model_label_lvl = widgets.IntSlider(min=0, max=2, description='Output Lvl Network:', value=0)\n",
    "    model_label_lvl.observe(create_label_remapper)\n",
    "    create_label_remapper()\n",
    "\n",
    "    def create_sliding_window_inferer(*args):\n",
    "        global SLIDING_WINDOW_INFERER\n",
    "        global STRIP_BACKGROUND\n",
    "\n",
    "        STRIP_BACKGROUND = strip_background_checkbox.value\n",
    "        print(f\"Strip Background: {STRIP_BACKGROUND}\")\n",
    "\n",
    "        use_sw = sliding_window_checkbox.value\n",
    "\n",
    "        if use_sw is None:\n",
    "            SLIDING_WINDOW_INFERER = None\n",
    "            print(\"Not using a sliding window inferer!\")\n",
    "            return\n",
    "\n",
    "        sw_avg = sliding_window_averaging_dropdown.value\n",
    "        sw_overlap = sliding_window_overlap_slider.value\n",
    "\n",
    "        SLIDING_WINDOW_INFERER = SlidingWindowInferer(roi_size=(512, 512), sw_batch_size=1, overlap=sw_overlap, mode=sw_avg)\n",
    "        print(f\"Created SLIDING_WINDOW_INFERER with overlap: {sw_overlap} and averaging: {sw_avg}.\")\n",
    "\n",
    "    create_sliding_window_inferer()\n",
    "    strip_background_checkbox.observe(create_sliding_window_inferer)\n",
    "    sliding_window_checkbox.observe(create_sliding_window_inferer)\n",
    "    sliding_window_averaging_dropdown.observe(create_sliding_window_inferer)\n",
    "    sliding_window_overlap_slider.observe(create_sliding_window_inferer)\n",
    "    sliding_window_menu.observe(create_sliding_window_inferer)\n",
    "\n",
    "    return statistics_dataset_menu, model_label_lvl, sliding_window_menu\n",
    "\n",
    "\n",
    "_dataset_menu, _label_remapper_menu, _sliding_window_menu = create_statistics_datset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Menus\n",
    "display(_model_menu)\n",
    "print(\"---------------\")\n",
    "display(_dataset_menu)\n",
    "print(\"---------------\")\n",
    "display(_label_remapper_menu)\n",
    "print(\"---------------\")\n",
    "display(_sliding_window_menu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model name:\", _model_menu.children[0].value)\n",
    "print(\"Data split:\", STATISTICS_DATASET.split)\n",
    "print(\"Label Level:\", STATISTICS_DATASET.label_level)\n",
    "print(\"Transform: \", _dataset_menu.children[3].value, \"Mask: \", _dataset_menu.children[4].value)\n",
    "print(\"SlidingWindow:\", _sliding_window_menu.children[0].value, \"Background Strip:\", _sliding_window_menu.children[1].value,\n",
    "      \"Combine: \", _sliding_window_menu.children[2].value, \"Overlap: \", _sliding_window_menu.children[3].value)\n",
    "print(type(model), type(model.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Forward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute outputs\n",
    "\n",
    "SUBSET = 1.0\n",
    "rand_subset = True\n",
    "\n",
    "dataloader = DataLoader(dataset=Subset(STATISTICS_DATASET, torch.randperm(len(STATISTICS_DATASET))[:int(len(STATISTICS_DATASET)*SUBSET)]),\n",
    "                        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "\n",
    "pix_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "max_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "pred_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "one_annotator_pred_freq = torch.zeros(num_classes, dtype=torch.int)\n",
    "\n",
    "activation_values_positive = [[] for _ in range(num_classes)]\n",
    "activation_values_negative = [[] for _ in range(num_classes)]\n",
    "\n",
    "activation_values_per_class = [[[] for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "\n",
    "\n",
    "conf_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n",
    "conf_matrix_ml = ConfusionMatrix(task=\"multilabel\", num_labels=num_classes)\n",
    "\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "\n",
    "    imgs, masks, background_masks = batch  # STATISTICS_DATASET[i]\n",
    "\n",
    "    outs = generate_model_output(model, imgs, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)        \n",
    "\n",
    "    for img, mask, background_mask, out in zip(imgs, masks, background_masks, outs):\n",
    "\n",
    "        out = torch.nn.functional.softmax(out, dim=0)\n",
    "\n",
    "        if STRIP_BACKGROUND:\n",
    "            foreground_mask = ~background_mask.bool()\n",
    "        else:\n",
    "            foreground_mask = torch.ones_like(background_mask).bool()\n",
    "\n",
    "        mask = mask[:, foreground_mask].flatten(start_dim=1)\n",
    "        out = out[:, foreground_mask].flatten(start_dim=1)\n",
    "\n",
    "        pix_freq += torch.sum((mask > 0), dim=(1))\n",
    "        max_freq += torch.bincount(torch.argmax(mask, dim=0).reshape(-1), minlength=num_classes)\n",
    "        pred_freq += torch.bincount(torch.argmax(out,  dim=0).reshape(-1), minlength=num_classes)\n",
    "        one_annotator_pred_freq += torch.sum(out >= 0.3333, dim=(1)) #torch.bincount(torch.argmax(out, dim=0).reshape(-1), minlength=num_classes)\n",
    "\n",
    "        conf_matrix(out.argmax(dim=0), mask.argmax(dim=0))\n",
    "        conf_matrix_ml(out.T >= 0.32, mask.T >= 0.32)\n",
    "        for cl in range(num_classes):\n",
    "\n",
    "            # aggr_mask_class = np.sum(np.stack([mask == cl for mask in masks]), axis=0) > 0\n",
    "\n",
    "            aggr_mask_class_pos = mask[cl, :] > 0\n",
    "            aggr_mask_class_neg = mask[cl, :] <= 0\n",
    "\n",
    "            rel_pixels = out[cl, aggr_mask_class_pos].detach().numpy()\n",
    "            sampled_pixels = np.random.choice(rel_pixels, int(rel_pixels.size/100))\n",
    "            activation_values_positive[cl].append(sampled_pixels)\n",
    "\n",
    "            rel_pixels = out[cl, aggr_mask_class_neg].detach().numpy()\n",
    "            sampled_pixels = np.random.choice(rel_pixels, int(rel_pixels.size/100))\n",
    "            activation_values_negative[cl].append(sampled_pixels)\n",
    "\n",
    "            sample_pixel_indices = np.random.permutation(int(aggr_mask_class_pos.sum()))\n",
    "            sample_pixel_indices = sample_pixel_indices[:int(len(sample_pixel_indices)/100)]\n",
    "            for cl2 in range(num_classes):\n",
    "\n",
    "                rel_pixels = out[cl2, aggr_mask_class_pos].detach().numpy().flatten()\n",
    "                sampled_pixels = rel_pixels[sample_pixel_indices]\n",
    "                activation_values_per_class[cl][cl2].append(sampled_pixels)\n",
    "\n",
    "\n",
    "for i in range(num_classes):\n",
    "    activation_values_positive[i] = np.concatenate(activation_values_positive[i])\n",
    "    activation_values_negative[i] = np.concatenate(activation_values_negative[i])\n",
    "\n",
    "    for j in range(num_classes):\n",
    "        activation_values_per_class[i][j] = np.concatenate(activation_values_per_class[i][j])\n",
    "\n",
    "\n",
    "num_activations = 2000\n",
    "\n",
    "activation_values_positive_resampled = [[] for _ in range(num_classes)]\n",
    "activation_values_negative_resampled = [[] for _ in range(num_classes)]\n",
    "activation_values_per_class_resampled = [[[] for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "\n",
    "\n",
    "for i in range(num_classes):\n",
    "    activation_values_positive_resampled[i] = activation_values_positive[i]\n",
    "    if len(activation_values_positive_resampled[i]) > num_activations:\n",
    "        activation_values_positive_resampled[i] = np.random.choice(activation_values_positive_resampled[i], size=num_activations, replace=False)\n",
    "    activation_values_negative_resampled[i] = activation_values_negative[i]\n",
    "\n",
    "    if len(activation_values_negative_resampled[i]) > num_activations:\n",
    "        activation_values_negative_resampled[i] = np.random.choice(activation_values_negative_resampled[i], size=num_activations, replace=False)\n",
    "\n",
    "    for j in range(num_classes):\n",
    "        activation_values_per_class_resampled[i][j] = activation_values_per_class[i][j]\n",
    "        if len(activation_values_per_class_resampled[i][j]) > num_activations:\n",
    "            activation_values_per_class_resampled[i][j] = np.random.choice(activation_values_per_class_resampled[i][j], size=num_activations, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "confm = conf_matrix.compute()\n",
    "\n",
    "confm_normed = confm / confm.sum(dim=1).reshape(-1,1)\n",
    "confm_normed = (confm_normed *1000).round().long()\n",
    "\n",
    "confm_expanded = torch.zeros(confm.shape[0]+1, confm.shape[1]+1, dtype=int)\n",
    "confm_expanded[:confm.shape[0], :confm.shape[1]] = confm_normed\n",
    "\n",
    "proportion_gt = (torch.sum(confm, dim=1) / torch.sum(confm)) * 1000\n",
    "proportion_predicted = (torch.sum(confm, dim=0) / torch.sum(confm)) * 1000\n",
    "\n",
    "\n",
    "# Calculate proportions\n",
    "\n",
    "confm_expanded[-1, :-1] = proportion_predicted.round().long()\n",
    "confm_expanded[:-1, -1] = proportion_gt.round().long()\n",
    "\n",
    "im = plt.matshow(confm_expanded)\n",
    "\n",
    "for (i, j), val in np.ndenumerate(confm_expanded):\n",
    "    plt.text(j, i, f'{val}', ha='center', va='center', color='red')\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.xticks(range(num_classes+1), list(map(lambda x: x[:20], classes_named)) + [\"Proportion\"],\n",
    "           rotation=45, rotation_mode=\"anchor\", ha=\"right\", va=\"center_baseline\")\n",
    "\n",
    "plt.yticks(range(num_classes+1), list(map(lambda x: x[:20], classes_named)) + [\"Proportion\"], rotation=45)\n",
    "plt.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"GT\")\n",
    "\n",
    "\n",
    "\n",
    "per_class_acc = ((confm.diagonal() / confm.sum(dim=1)) * 1000).round().long()\n",
    "\n",
    "r = torch.stack([per_class_acc, proportion_predicted.round().long(), proportion_gt.round().long()])\n",
    "im2 = plt.matshow(r)\n",
    "plt.colorbar(im2)\n",
    "for (i,j), val in np.ndenumerate(r):\n",
    "    plt.text(j,i, f\"{val}\", c=\"red\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm_ml = conf_matrix_ml.compute()\n",
    "confm_ml_reshape = confm_ml.reshape(confm_ml.shape[0], -1)\n",
    "num_pixels = confm_ml_reshape.sum(dim=1)\n",
    "confm_ml_reshape_normed = confm_ml_reshape / num_pixels.reshape(-1, 1)\n",
    "\n",
    "TN, FP, FN, TP = confm_ml_reshape[:, 0], confm_ml_reshape[:, 1], confm_ml_reshape[:, 2] ,confm_ml_reshape[:, 3] \n",
    "\n",
    "POPU = (TN+FP+FN+TP)\n",
    "ACC = (TN+TP)/POPU\n",
    "TPR = TP/(TP+FN)\n",
    "TNR = (TN/(TN+FP))\n",
    "PREC = TP/(FP+TP)\n",
    "BACC = (TPR+TNR) /2\n",
    "\n",
    "PREL = (TP+FN)/POPU\n",
    "\n",
    "confm_ml_reshape_normed = torch.cat([PREL.reshape(num_classes,1), confm_ml_reshape_normed, ACC.reshape(num_classes, 1), PREC.reshape(num_classes,1), BACC.reshape(num_classes,1)], dim=1)\n",
    "\n",
    "im3 = plt.matshow(confm_ml_reshape_normed)\n",
    "plt.colorbar(im3)\n",
    "plt.xticks([0,1,2,3,4,5,6,7], [\"PREL\", \"TN\", \"FP\", \"FN\", \"TP\", \"ACC\", \"PREC\", \"BA\"], rotation=45)\n",
    "_ = plt.yticks(range(len(classes_named)),classes_named)\n",
    "_ = plt.xlabel(\"Metrics in %\")\n",
    "for (i,j), val in np.ndenumerate(confm_ml_reshape_normed):\n",
    "    plt.text(j,i, f\"{val*100:0.0f}\",c=\"red\", ha=\"center\", va=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class annotation and prediction frequency\n",
    "bar_width = 0.2\n",
    "plt.bar(np.arange(num_classes)-1.5*bar_width, pix_freq/torch.sum(max_freq), label=\"at least one annotaor\", width=bar_width)\n",
    "plt.bar(np.arange(num_classes)-0.5*bar_width, max_freq/torch.sum(max_freq), label=\"majority vote\", width=bar_width)\n",
    "#plt.bar(np.arange(num_classes)+1.5*bar_width, one_annotator_pred_freq/torch.sum(max_freq), label=\"one_annotator_pred_freq\", width=bar_width)\n",
    "plt.bar(np.arange(num_classes)+0.5*bar_width, pred_freq/torch.sum(pred_freq), label=\"pred_freq\", width=bar_width)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "_ = plt.legend()\n",
    "_ = plt.xticks(np.arange(num_classes), list(map(lambda x: x[:20], classes_named)), rotation=90)\n",
    "\n",
    "# inv_weights = 1/(max_freq/torch.sum(max_freq))\n",
    "# inv_weights /= torch.sum(inv_weights)\n",
    "# print(inv_weights.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Softmax Values per Class\n",
    "plt.hist(np.concatenate(activation_values_positive), label=\"all\", histtype=\"step\", density=False, bins=100, color=(0, 0, 0))\n",
    "plt.hist(activation_values_positive, label=classes_named, histtype=\"step\", density=False, bins=100)\n",
    "_ = plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of softmax values over all classes per GT class\n",
    "\n",
    "def exl_idx(arr, idx):\n",
    "    return arr[:idx]+arr[idx+1:]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6))\n",
    "fig.set_facecolor((0.0, 0., 0., 0.))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for cl_idx, ax in enumerate(axes):\n",
    "\n",
    "    ax.hist(exl_idx(activation_values_per_class[cl_idx], cl_idx)[::-1]+[activation_values_per_class[cl_idx][cl_idx]], label=exl_idx(classes_named, cl_idx)\n",
    "            [::-1]+[f\"TrueClass\"], bins=100, histtype=\"step\", density=True, color=exl_idx(list(STATISTICS_DATASET.colormap.colors), cl_idx)[::-1]+[(0.8, 0.2, 0.7)])\n",
    "    ax.set_title(classes_named[cl_idx])\n",
    "    ax.set_facecolor((.0, 0., 0.))\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tetrahedron Plot\n",
    "\n",
    "\n",
    "def plot_tetrahedron(dataset, softmaxes, correct_class=0):\n",
    "    # Define tetrahedron vertices\n",
    "\n",
    "    # vertices = np.array([[0, 0, 0],\n",
    "    #                     [1, 0, 0],\n",
    "    #                     [0, 1, 0],\n",
    "    #                     [0, 0, 1],\n",
    "    #                     [1/3, 1/3, 1/3]])\n",
    "\n",
    "    # inv_sq2 = 1/math.sqrt(2)\n",
    "    # vertices = np.array([[1, 0, -inv_sq2],\n",
    "    #                     [-1, 0, -inv_sq2],\n",
    "    #                     [0, 1, inv_sq2],\n",
    "    #                     [0, -1, inv_sq2],\n",
    "    #                     [1/3, 1/3, 1/3]])\n",
    "\n",
    "    vertices = np.array([[1, 1, 1],\n",
    "                         [1, -1, -1],\n",
    "                         [-1, 1, -1],\n",
    "                         [-1, -1, 1],\n",
    "                         [0, 0, 0]])\n",
    "\n",
    "    vertices[-1] = np.mean(vertices[:-1])\n",
    "\n",
    "    # Define edges\n",
    "    edges = [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3], [0, 4], [1, 4], [2, 4], [3, 4]]\n",
    "\n",
    "    # Define trace for vertices\n",
    "    trace_vertices = go.Scatter3d(\n",
    "        x=vertices[:, 0],\n",
    "        y=vertices[:, 1],\n",
    "        z=vertices[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color='red'),\n",
    "        text=classes_named[1:]\n",
    "    )\n",
    "\n",
    "    corr_vertex = go.Scatter3d(\n",
    "        x=[vertices[correct_class, 0]],\n",
    "        y=[vertices[correct_class, 1]],\n",
    "        z=[vertices[correct_class, 2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color='green'),\n",
    "        text=classes_named[1:][correct_class]\n",
    "    )\n",
    "\n",
    "    # Define trace for edges\n",
    "    traces_edges = []\n",
    "    for edge in edges:\n",
    "        trace_edge = go.Scatter3d(\n",
    "            x=[vertices[edge[0], 0], vertices[edge[1], 0]],\n",
    "            y=[vertices[edge[0], 1], vertices[edge[1], 1]],\n",
    "            z=[vertices[edge[0], 2], vertices[edge[1], 2]],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=2))\n",
    "        traces_edges.append(trace_edge)\n",
    "\n",
    "    softmaxes = np.stack(softmaxes, axis=1)\n",
    "\n",
    "    rnd_idcs = np.random.permutation(softmaxes.shape[0])[:5000]\n",
    "\n",
    "    softmaxes = softmaxes[rnd_idcs, :]\n",
    "\n",
    "    car_coords = softmaxes @ vertices[:-1]\n",
    "\n",
    "    preds = go.Scatter3d(\n",
    "        x=car_coords[:, 0],\n",
    "        y=car_coords[:, 1],\n",
    "        z=car_coords[:, 2],\n",
    "        mode='markers',\n",
    "        # marker=dict(size=2, color=np.array([0.5, 0.5, 0.0])),   #[dict(size=2, color=(1-sm[correct_class])*np.array([1., 0., 0.]) + sm[correct_class]*np.array([0., 1., 0.])) for sm in softmaxes],\n",
    "        text=[f\"({sm[0]:.2f},{sm[1]:.2f},{sm[2]:.2f},{sm[3]:.2f})\" for sm in softmaxes],\n",
    "        marker_size=2,\n",
    "        marker_color=[np.array([0., 1., 0.]) if sm.argmax() == correct_class else np.array([1., 0., 0.]) for sm in softmaxes]\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[trace_vertices, corr_vertex, preds, *traces_edges])\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(title='GP 3'),\n",
    "            yaxis=dict(title='GP 4'),\n",
    "            zaxis=dict(title='GP 5')\n",
    "        ),\n",
    "        title='Tetrahedron Plot'\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "tetrahedron_class_selecter = widgets.ToggleButtons(options=classes_named, index=0)\n",
    "plot_tetrahedron(STATISTICS_DATASET, activation_values_per_class[0], correct_class=0)\n",
    "tetrahedron_class_selecter.observe(lambda x: plot_tetrahedron(\n",
    "    STATISTICS_DATASET, activation_values_per_class[tetrahedron_class_selecter.index], correct_class=tetrahedron_class_selecter.index))\n",
    "\n",
    "\n",
    "display(tetrahedron_class_selecter)\n",
    "\n",
    "# Example usage\n",
    "# Replace classes_probs with your predictive probabilities for each class\n",
    "# plot_tetrahedron(STATISTICS_DATASET, activation_values_per_class[class_idx], correct_class=class_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_dist_with_thres(dataset, activation_values, thresholds, norm_hist=True, thresholds_are_values=True):\n",
    "\n",
    "    def resample(arr, num):\n",
    "\n",
    "        if len(arr) > num:\n",
    "            arr = np.random.choice(arr, size=num, replace=False)\n",
    "        return arr\n",
    "\n",
    "    activation_values_positive, activation_values_negative = activation_values\n",
    "\n",
    "    print(list(map(lambda x: x is not None, activation_values_positive)))\n",
    "    if not thresholds_are_values:\n",
    "        thresholds = [np.percentile(activation_values_positive[i], thresholds[i]*100) for i in range(len(thresholds))]\n",
    "\n",
    "    # Create a 2x5 grid of histograms\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=math.ceil(len(thresholds)/2), figsize=(15, 6))\n",
    "\n",
    "    # Flatten the 2D array of axes for easier iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(0, len(thresholds)):\n",
    "        # Plot histogram on the corresponding subplot\n",
    "\n",
    "        if norm_hist:\n",
    "\n",
    "            if activation_values_positive[i] is not None:\n",
    "                axes[i].hist([activation_values_positive[i],], bins=100,\n",
    "                             histtype=\"step\", density=norm_hist, label=\"positive\", color=\"g\")  # , range=(0.0, 1.0))\n",
    "            if activation_values_negative[i] is not None:\n",
    "                axes[i].hist([activation_values_negative[i],], bins=100,\n",
    "                             histtype=\"step\", density=norm_hist, label=\"negative\", color=\"r\")  # , range=(0.0, 1.0))\n",
    "\n",
    "        else:\n",
    "            if activation_values_positive[i] is not None:\n",
    "                axes[i].hist([activation_values_positive[i],], bins=100,\n",
    "                             histtype=\"step\", density=norm_hist, label=\"positive\", color=\"g\")  # , range=(0.0, 1.0))\n",
    "            if activation_values_negative[i] is not None:\n",
    "                axes[i].hist([activation_values_negative[i],], bins=100,\n",
    "                             histtype=\"step\", density=norm_hist, label=\"negative\", color=\"r\")  # , range=(0.0, 1.0))\n",
    "\n",
    "        # if activation_values_positive[i] is not None and activation_values_negative[i] is not None:\n",
    "        #    axes[i].text(0.95, 0.95, \"ration: \" +str(len(activation_values_negative[i])/len(activation_values_positive[i])), transform=axes[i].transAxes, ha='right', va='top')\n",
    "\n",
    "        axes[i].axvline(x=thresholds[i], color='red', linestyle='--', label='Quantile Threshold')\n",
    "\n",
    "        title = \"benign\" if i == 0 else dataset.explanations[i-1]\n",
    "        axes[i].set_title(title[:20])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_sliders = {}\n",
    "threshold_sliders[\"Benign\"] = ipywidgets.FloatSlider(min=0, max=1, step=0.01, value=0.1, description=f'Benign', continuous_update=False)\n",
    "for cl, idx in STATISTICS_DATASET.exp_number_mapping.items():\n",
    "    threshold_sliders[cl] = ipywidgets.FloatSlider(min=0, max=1, step=0.01, value=0.1, description=f'{cl}', continuous_update=False)\n",
    "\n",
    "vis_mode_widget = ipywidgets.widgets.ToggleButtons(\n",
    "    options=[\"contour\", \"contourf\", \"heatmap\", \"multilabel\", \"thresholded\"],\n",
    "    value=\"contourf\",\n",
    "    description=\"Visualization mode\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "strip_background_button = ipywidgets.widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Strip Background',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "idx_slider = ipywidgets.widgets.IntSlider(min=0, max=len(STATISTICS_DATASET), step=1, value=0)\n",
    "\n",
    "\n",
    "normed_hist_check = ipywidgets.widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Norm Hist',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "show_hist_check = ipywidgets.widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Show Hist',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "\n",
    "show_images_check = ipywidgets.widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Show Img',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "use_per_image_thres_check = ipywidgets.widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Use per img thres',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "dummy_checkbox = ipywidgets.widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Hidden',\n",
    "    disabled=True,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "dummy_checkbox.layout.visibility = \"hidden\"\n",
    "\n",
    "stack = ipywidgets.widgets.Stack([dummy_checkbox, dummy_checkbox, dummy_checkbox, dummy_checkbox,wid.HBox([wid.VBox(\n",
    "    [show_hist_check, normed_hist_check, show_images_check, use_per_image_thres_check]), wid.VBox([slider for slider in threshold_sliders.values()])])], selected_index=0)\n",
    "ipywidgets.widgets.jslink((vis_mode_widget, 'index'), (stack, 'selected_index'))\n",
    "vis_mode_selection = ipywidgets.widgets.VBox([vis_mode_widget, stack])\n",
    "\n",
    "interface = wid.VBox([idx_slider, strip_background_button, vis_mode_selection])\n",
    "\n",
    "# Predictions\n",
    "# @ipywidgets.interact(idx=idx_slider, vis_mode=vis_mode_widget, norm_hist=normed_hist_check, show_img=show_images_check, ** threshold_sliders)\n",
    "\n",
    "\n",
    "def vis_thresholds_interactive(idx, vis_mode, norm_hist, show_img, show_hist, use_per_image_thres, strip_background, **thresholds):\n",
    "\n",
    "    dataset = STATISTICS_DATASET\n",
    "\n",
    "    if vis_mode == \"thresholded\":\n",
    "\n",
    "        img, mask, background_mask = dataset[idx]\n",
    "        out = model(img.unsqueeze(0)).squeeze(0)\n",
    "        out = torch.nn.functional.softmax(out, dim=0).detach()\n",
    "        org_img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(dataset.get_raw_image(idx)))[\"image\"]\n",
    "\n",
    "        if use_per_image_thres:\n",
    "            vis_activation_values_pos = []\n",
    "            vis_activation_values_neg = []\n",
    "\n",
    "        for cl in range(dataset.num_classes):\n",
    "\n",
    "            aggr_mask_class_pos = mask[cl, :, :] > 0\n",
    "            aggr_mask_class_neg = mask[cl, :, :] <= 0\n",
    "\n",
    "            if aggr_mask_class_pos.sum() > 0:\n",
    "                rel_pixels = out[cl, aggr_mask_class_pos].detach().numpy().flatten()\n",
    "            else:\n",
    "                rel_pixels = None\n",
    "\n",
    "            vis_activation_values_pos.append(rel_pixels)\n",
    "\n",
    "            if aggr_mask_class_neg.sum() > 0:\n",
    "                rel_pixels = out[cl, aggr_mask_class_neg].detach().numpy().flatten()\n",
    "            else:\n",
    "                rel_pixels = None\n",
    "\n",
    "            vis_activation_values_neg.append(rel_pixels)\n",
    "\n",
    "        else:\n",
    "            vis_activation_values_pos = activation_values_positive\n",
    "            vis_activation_values_neg = activation_values_negative\n",
    "\n",
    "        thresholds_list = list(thresholds.values())\n",
    "\n",
    "        value_thresholds = [(np.percentile(vis_activation_values_pos[i], thresholds_list[i]*100)\n",
    "                             if vis_activation_values_pos[i] is not None else 0.0) for i in range(dataset.num_classes)]\n",
    "        if show_hist:\n",
    "            plot_class_dist_with_thres(dataset, activation_values=(vis_activation_values_pos, vis_activation_values_neg),\n",
    "                                       thresholds=value_thresholds, norm_hist=norm_hist)\n",
    "        if show_img:\n",
    "            create_single_class_acti_maps(dataset=dataset, idx=idx, model=model, plot_mode=vis_mode,\n",
    "                                          strip_background=strip_background, thresholds=value_thresholds)\n",
    "    else:\n",
    "        create_single_class_acti_maps(dataset=dataset, idx=idx, model=model, plot_mode=vis_mode,\n",
    "                                      strip_background=strip_background, thresholds=None)\n",
    "\n",
    "\n",
    "out = wid.interactive_output(vis_thresholds_interactive, {\"idx\": idx_slider, \"vis_mode\": vis_mode_widget, \"norm_hist\": normed_hist_check, \"show_img\": show_images_check,\n",
    "                             \"show_hist\": show_hist_check, \"use_per_image_thres\": use_per_image_thres_check, \"strip_background\": strip_background_button, **threshold_sliders})\n",
    "\n",
    "show_widget = wid.VBox([interface, out])\n",
    "show_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(idx=widgets.IntSlider(min=0, max=len(STATISTICS_DATASET), value=0), strip_background=True, class_activation_map=False, norm_by_class_freq=False, create_segmentation=True)\n",
    "def interactive_composite_plot(idx, strip_background, class_activation_map, norm_by_class_freq, create_segmentation):\n",
    "\n",
    "    dataset = STATISTICS_DATASET\n",
    "    if not class_activation_map:\n",
    "        #composite_prediction_plot(dataset=dataset, model=model, indices=[idx], mask_background=strip_background)\n",
    "\n",
    "        img, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "        org_img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(dataset.get_raw_image(idx)))[\"image\"]\n",
    "\n",
    "        background = background_mask if strip_background else None\n",
    "\n",
    "        if model is not None and create_segmentation:\n",
    "            out = generate_model_output(model, img, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)\n",
    "\n",
    "\n",
    "            out = torch.nn.functional.softmax(out, 0)\n",
    "            if norm_by_class_freq:\n",
    "                out /= torch.tensor([0.38, .21, .25, .17]).reshape(-1, 1, 1)\n",
    "            np_seg = np.array(out.argmax(dim=0)).astype(np.uint8)\n",
    "            f = create_composite_plot(dataset, org_img,  {\"segmentation\": np_seg} | {f\"Annotator {i}\": mask for i,\n",
    "                                    mask in enumerate(masks)}, background, only_show_existing_annotation=True)\n",
    "        else:\n",
    "            f = create_composite_plot(dataset, org_img, {f\"Annotator {i}\": mask for i, mask in enumerate(masks)},\n",
    "                                    background, only_show_existing_annotation=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        create_single_class_acti_maps(dataset=dataset, idx=idx, model=model, plot_mode=\"heatmap\",\n",
    "                                      strip_background=strip_background, thresholds=None)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_composite_plot(209, True, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(idx=widgets.IntSlider(0,0, len(STATISTICS_DATASET)))\n",
    "def bbb(idx):\n",
    "    f = composite_prediction_plot(None, STATISTICS_DATASET, [idx], mask_background=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(idx=widgets.IntSlider(0, 0, len(STATISTICS_DATASET)))\n",
    "def vis_topdown_diff(idx):\n",
    "    img, mask, background = STATISTICS_DATASET[idx]\n",
    "    preds = generate_model_output(model, img, device=device, inferer=SLIDING_WINDOW_INFERER)\n",
    "    lvl0_cmap = get_class_colormaps({\"3\": 1,\"4\":1,\"5\":1})\n",
    "    lvl1_cmap = get_class_colormaps({\"3\": 2,\"4\":3,\"5\":4})\n",
    "\n",
    "\n",
    "    lvl_0_cmap_background = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), lvl0_cmap.colors]))\n",
    "    lvl_1_cmap_background = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), lvl1_cmap.colors]))\n",
    "\n",
    "    lvl_0_preds_torch, lvl_1_preds_torch = generate_label_hierarchy(preds.cpu().unsqueeze(0), STATISTICS_DATASET.exp_numbered_lvl_remapping, start_level=1)\n",
    "    lvl_0_preds = lvl_0_preds_torch.squeeze(0).argmax(dim=0).numpy().astype(np.uint8)\n",
    "    lvl_1_preds = lvl_1_preds_torch.squeeze(0).argmax(dim=0).numpy().astype(np.uint8)\n",
    "\n",
    "    f, ax = plt.subplots(2,2)\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    def mask_background(mask, background_mask):\n",
    "        mask = mask + 1 \n",
    "        mask[background_mask] = 0.0\n",
    "        return mask\n",
    "\n",
    "    lvl_0_preds_masked = mask_background(lvl_0_preds, background)\n",
    "    lvl_1_preds_masked = mask_background(lvl_1_preds, background)\n",
    "    for a in ax:\n",
    "        a.set_axis_off()\n",
    "\n",
    "    def get_topdown_classes(pred0, pred1):\n",
    "\n",
    "        argmax_preds0 = pred0.argmax(dim=0)\n",
    "\n",
    "        argmax_lvl_1 = torch.zeros_like(argmax_preds0)\n",
    "\n",
    "        lvl_0_1_remapping = STATISTICS_DATASET.exp_numbered_lvl_remapping[0]\n",
    "\n",
    "        for lvl_0_class, lvl_1_classes in lvl_0_1_remapping.items():\n",
    "\n",
    "            where_in_lvl_0 = argmax_preds0 == lvl_0_class\n",
    "            argmax_lvl1_selected_classes = pred1[lvl_1_classes, :, :].argmax(dim=0)\n",
    "            \n",
    "            lvl_1_classes = torch.tensor(lvl_1_classes).reshape(-1,1,1)\n",
    "            #assert lvl_1_classes.dim() == argmax_lvl1_selected_classes.dim()\n",
    "            #Remap through gather\n",
    "            argmax_lvl1_remapped = torch.gather(lvl_1_classes.expand((lvl_1_classes.shape[0],*argmax_lvl1_selected_classes.shape)),\n",
    "                                                0, argmax_lvl1_selected_classes.unsqueeze(0)).squeeze(0)\n",
    "            argmax_lvl_1[where_in_lvl_0] = argmax_lvl1_remapped[where_in_lvl_0]\n",
    "        \n",
    "        return argmax_preds0, argmax_lvl_1\n",
    "\n",
    "    _, lvl_1_preds_topdown = get_topdown_classes(lvl_0_preds_torch.squeeze(0), lvl_1_preds_torch.squeeze(0))\n",
    "    lvl_1_preds_topdown = lvl_1_preds_topdown.numpy().astype(np.uint8)\n",
    "    lvl_1_preds_topdown_masked = mask_background(lvl_1_preds_topdown, background)\n",
    "\n",
    "\n",
    "\n",
    "    ax[0].imshow(lvl_0_preds_masked, cmap=lvl_0_cmap_background, vmin=0, vmax=len(lvl_0_cmap_background.colors)-1, interpolation_stage=\"rgba\")\n",
    "    ax[1].imshow(lvl_1_preds_masked, cmap=lvl_1_cmap_background, vmin=0, vmax=len(lvl_1_cmap_background.colors)-1, interpolation_stage=\"rgba\")\n",
    "    ax[2].imshow(lvl_1_preds_topdown_masked, cmap=lvl_1_cmap_background, vmin=0, vmax=len(lvl_1_cmap_background.colors)-1, interpolation_stage=\"rgba\")\n",
    "    ax[3].imshow(lvl_1_preds_topdown_masked != lvl_1_preds_masked, interpolation_stage=\"rgba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_topdown_diff(209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATISTICS_DATASET.exp_numbered_lvl_remapping[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_label_hierarchy(preds[:, 0,0].unsqueeze(0),STATISTICS_DATASET.exp_numbered_lvl_remapping, start_level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(np.array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "composite_prediction_plot(None, STATISTICS_DATASET, [13, 25,26,28, 37], mask_background=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    num_classes = dataloader.dataset.num_classes\n",
    "\n",
    "    accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes, average=\"micro\")\n",
    "    per_class_accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes, average=\"none\")\n",
    "    class_occurence = torch.zeros(num_classes)\n",
    "    class_predictions = torch.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            labels = torch.argmax(labels, dim=1)  # Convert soft labels to hard labels\n",
    "\n",
    "            class_occurence += torch.bincount(labels.reshape(-1), minlength=num_classes)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu()\n",
    "\n",
    "            class_predictions += torch.bincount(predictions.reshape(-1), minlength=num_classes)\n",
    "\n",
    "            accuracy_metric(predictions, labels)\n",
    "            per_class_accuracy_metric(predictions, labels)\n",
    "\n",
    "    overall_accuracy = accuracy_metric.compute()\n",
    "    per_class_accuracy = per_class_accuracy_metric.compute()\n",
    "\n",
    "    return overall_accuracy, per_class_accuracy, class_occurence, class_predictions\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "# Example usage:\n",
    "overall_accuracy, per_class_accuracy, class_freq, pred_freq = compute_accuracy(model, dataloader_Gleason, device)\n",
    "\n",
    "print(f\"Class frequency: {class_freq}\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy}\")\n",
    "print(\"Per-Class Accuracy:\")\n",
    "for class_idx, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Class {class_idx}: {acc.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.25\n",
    "\n",
    "plt.bar(np.arange(len(class_freq))-bar_width/2, class_freq/torch.sum(class_freq), width=bar_width, label=\"Label Frequency\")\n",
    "plt.bar(np.arange(len(pred_freq))+bar_width/2, pred_freq/torch.sum(pred_freq), width=bar_width, label=\"Prediction frequency\")\n",
    "plt.legend()\n",
    "# plt.xticks(range(len(pred_freq)), list(map(lambda x: x[:20], CLASSES_NAMED)), rotation=90)\n",
    "plt.yscale(\"linear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_org = GleasonX(DATA_PATH, \"train\", scaling=\"original\", transforms=data_transform, label_level=LABEL_LEVEL)\n",
    "max_pix = 0\n",
    "min_pix = 1000000000\n",
    "shape = None\n",
    "m_shape = None\n",
    "for slide in dataset_org.used_slides:\n",
    "\n",
    "    path = dataset_org.tma_base_path/dataset_org.tma_paths[slide]\n",
    "\n",
    "    img = Image.open(path)\n",
    "\n",
    "    pix_size = img.size\n",
    "\n",
    "    num_pixs = pix_size[0]*pix_size[1]\n",
    "\n",
    "    if num_pixs > max_pix:\n",
    "        max_pix = num_pixs\n",
    "        shape = pix_size\n",
    "\n",
    "    if num_pixs < min_pix:\n",
    "        min_pix = num_pixs\n",
    "        m_shape = pix_size\n",
    "    print(slide)\n",
    "print(max_pix)\n",
    "print(shape)\n",
    "\n",
    "print(min_pix)\n",
    "print(m_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_512_full_scale = alb.Compose([\n",
    "    alb.augmentations.geometric.resize.SmallestMaxSize(max_size=512, interpolation=2, always_apply=False, p=1),\n",
    "    alb.CenterCrop(width=512, height=512, p=1),\n",
    "])\n",
    "\n",
    "\n",
    "resize_512_scale025 = alb.Compose([\n",
    "    alb.augmentations.geometric.resize.SmallestMaxSize(max_size=2048, interpolation=2, always_apply=False, p=1),\n",
    "    alb.RandomResizedCrop(height=512, width=512, scale=(.1, .1), interpolation=2),\n",
    "    # alb.RandomRotate90(p=1),\n",
    "])\n",
    "\n",
    "resize_512_scale05 = alb.Compose([\n",
    "    alb.augmentations.geometric.resize.SmallestMaxSize(max_size=2048, interpolation=2, always_apply=False, p=1),\n",
    "    alb.RandomResizedCrop(height=512, width=512, scale=(0.5, 0.5), interpolation=2),\n",
    "    # alb.RandomRotate90(p=1),\n",
    "])\n",
    "\n",
    "# TODO das stimmt nocht nicht. Zoome ich jetzt rein oder raus?\n",
    "rnd_scale = alb.Compose([alb.augmentations.geometric.resize.SmallestMaxSize(max_size=2048, interpolation=2, always_apply=False, p=1),\n",
    "                         alb.RandomScale(scale_limit=(1+-0.5, 1+-0.5), p=1.0),\n",
    "                         alb.RandomCrop(512, 512, p=1.0)])\n",
    "\n",
    "# dataset_for_vis = GleasonX(DATA_PATH, \"val\", scaling=\"original\", transforms=rnd_scale, label_level=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _convert_to_random_scale_tuple(input):\n",
    "\n",
    "    if isinstance(input, (float, int)):\n",
    "        input = (input, input)\n",
    "\n",
    "    if not isinstance(input, np.ndarray):\n",
    "        input = np.array(input)\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "def create_scaling_crop(scale_factor, image_resize=2048, patch_size=512, crop=\"random\"):\n",
    "\n",
    "    assert crop in [\"random\", \"center\", None]\n",
    "\n",
    "    if crop == \"random\":\n",
    "        crop = alb.RandomCrop(patch_size, patch_size, p=1.0)\n",
    "    elif crop == \"center\":\n",
    "        crop = alb.CenterCrop(patch_size, patch_size, p=1.0)\n",
    "    else:\n",
    "        crop = alb.Identity()\n",
    "\n",
    "    scale_factor = _convert_to_random_scale_tuple(scale_factor)\n",
    "\n",
    "    # alb.RandomScale uses a bias of 1. For whatever reason. So scaling the image by 0.25 actually requires the input -0.75 wtf. Oh and it wants tuples.\n",
    "    scale_factor -= 1\n",
    "    scale_factor = tuple(scale_factor)\n",
    "\n",
    "    scale_only = alb.Compose([alb.augmentations.geometric.resize.SmallestMaxSize(max_size=image_resize, interpolation=2, always_apply=False, p=1.0),\n",
    "                              alb.RandomScale(scale_limit=scale_factor, p=1.0),\n",
    "                              crop], p=1.0)\n",
    "\n",
    "    return scale_only\n",
    "\n",
    "\n",
    "def create_zoom_crop(zoom_factor, image_resize=2048, patch_size=512, crop=\"random\"):\n",
    "\n",
    "    zoom_factor = _convert_to_random_scale_tuple(zoom_factor)\n",
    "\n",
    "    scale_factor = (zoom_factor * patch_size)/image_resize\n",
    "\n",
    "    aug = create_scaling_crop(scale_factor, image_resize, patch_size, crop)\n",
    "    return aug\n",
    "\n",
    "\n",
    "def create_fraction_of_border_crop(border_fraction, image_resize=2048, patch_size=512, crop=\"random\"):\n",
    "\n",
    "    border_fraction = _convert_to_random_scale_tuple(border_fraction)\n",
    "    zoom_factor = 1/border_fraction\n",
    "\n",
    "    return create_zoom_crop(zoom_factor=zoom_factor, image_resize=image_resize, patch_size=patch_size, crop=crop)\n",
    "\n",
    "\n",
    "def create_fraction_of_image_crop(image_fraction, image_resize=2048, patch_size=512, crop=\"random\"):\n",
    "\n",
    "    image_fraction = _convert_to_random_scale_tuple(image_fraction)\n",
    "\n",
    "    border_length_fraction = np.sqrt(image_fraction)\n",
    "\n",
    "    return create_fraction_of_border_crop(border_fraction=border_length_fraction, image_resize=image_resize, patch_size=patch_size, crop=crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_scale_in_train_transform(croping_transform):\n",
    "    return alb.Compose([\n",
    "        # alb.RandomBrightnessContrast(p=0.25),33\n",
    "\n",
    "        # alb.RandomResizedCrop(height=512, width=512, scale=(0.25, 1), interpolation=2),\n",
    "        croping_transform,\n",
    "        # Basic\n",
    "        alb.HorizontalFlip(p=0.5),\n",
    "        alb.RandomRotate90(p=1),\n",
    "\n",
    "        # Morphology (scaling is already included in RandomResizedCrop)\n",
    "        alb.OneOf([\n",
    "            alb.ElasticTransform(),\n",
    "            alb.GridDistortion(),], p=0.25),\n",
    "\n",
    "        # Blur or Noise\n",
    "        alb.OneOf([\n",
    "            alb.AdvancedBlur(p=.25),\n",
    "            alb.GaussNoise()], p=0.25),\n",
    "\n",
    "        # Includes Brightness, Contrast, Hue and Saturation\n",
    "        alb.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n",
    "\n",
    "        # Normalization\n",
    "        alb.Normalize(),\n",
    "        # alb.augmentations.geometric.resize.SmallestMaxSize(max_size=512, interpolation=2, always_apply=False, p=1),\n",
    "        # alb.RandomCrop(width=512, height=512, p=1)\n",
    "\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_random_resized_crop(s, image_resize=2048):\n",
    "\n",
    "    return alb.Compose([alb.augmentations.geometric.resize.SmallestMaxSize(max_size=image_resize, interpolation=2, always_apply=False, p=1.0),\n",
    "                        alb.RandomResizedCrop(height=512, width=512, scale=(s, s), interpolation=2),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = STATISTICS_DATASET\n",
    "idx = 2\n",
    "strip_background = True\n",
    "\n",
    "transforms = dataset.tissue_mask_kwargs.copy()\n",
    "dataset.tissue_mask_kwargs = {\"open\": False, \"close\": False, \"flood\": False}\n",
    "img, masks, background_mask = dataset.__getitem__(idx, False)\n",
    "# org_img = augmentations.basic_transforms_val_test_colorpreserving(image=np.array(dataset.get_raw_image(idx)))[\"image\"]\n",
    "\n",
    "out = generate_model_output(model, img, device=device, label_remapping=LABEL_REMAPPING, inferer=SLIDING_WINDOW_INFERER)\n",
    "\n",
    "out = torch.nn.functional.softmax(out, 0)\n",
    "\n",
    "np_seg = np.array(out.argmax(dim=0)).astype(np.uint8)\n",
    "\n",
    "org_img = np.array(dataset.get_raw_image(idx).resize(np_seg.shape))\n",
    "\n",
    "colormap = ListedColormap(dataset.colormap.colors)\n",
    "num_class_to_vis = dataset.num_classes\n",
    "\n",
    "if strip_background:\n",
    "\n",
    "    for mask in masks:\n",
    "        mask += 1\n",
    "        mask[background_mask] = 0\n",
    "\n",
    "    np_seg += 1\n",
    "    np_seg[background_mask] = 0\n",
    "\n",
    "    colormap = ListedColormap(np.concatenate([np.array([[0., 0., 0., 1.]]), dataset.colormap.colors]))\n",
    "    num_class_to_vis = dataset.num_classes + 1\n",
    "\n",
    "f, axes = plt.subplots(2, 3+math.ceil(dataset.num_classes/2), sharex=False, sharey=False, constrained_layout=False, figsize=(12, 4))\n",
    "f.tight_layout()\n",
    "\n",
    "\n",
    "axes[0, 0].imshow(org_img)\n",
    "axes[0, 0].set_title(\"Image\", size=7)\n",
    "axes[0, 0].set_axis_off()\n",
    "\n",
    "# axes[1, 0].imshow(img)\n",
    "axes[1, 0].imshow(np_seg.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1, interpolation=\"nearest\")\n",
    "axes[1, 0].set_title(\"Segmentation\", size=7)\n",
    "axes[1, 0].set_axis_off()\n",
    "\n",
    "for sub_ax, mask in zip_longest(list(axes[:, 1:3].flatten()), masks):\n",
    "    sub_ax.set_axis_off()\n",
    "\n",
    "    if mask is not None:\n",
    "        sub_ax.imshow(mask.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1, interpolation=\"nearest\")\n",
    "        sub_ax.set_title(\"Annotation\", size=7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask in masks:\n",
    "    print(np.unique(mask.astype(int)))\n",
    "\n",
    "print(np.unique(np_seg))\n",
    "print(num_class_to_vis-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(size=widgets.FloatSlider(7, min=0.0, max=7, step=.4))\n",
    "def show_mat_bug(size):\n",
    "    mask = masks[0]\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(mask.astype(int), cmap=colormap, vmin=0, vmax=num_class_to_vis - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.ones((100, 100))\n",
    "img[0:50, :] = 2\n",
    "plt.figure(figsize=(2, 1))\n",
    "plt.imshow(img, cmap=colormap, vmin=0, vmax=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GleasonXAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
